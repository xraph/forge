---
title: "Queue Extension"
description: "Comprehensive guide to Forge v2's Queue extension for reliable message queuing and background job processing"
---

import { Tabs, Tab } from 'fumadocs-ui/components/tabs';
import { Callout } from 'fumadocs-ui/components/callout';

# Queue Extension

The Queue extension provides a robust, scalable message queuing system for background job processing, task scheduling, and reliable message delivery in Forge v2 applications.

## Key Features

- **Multiple Queue Backends**: Redis, In-Memory, Database, SQS, RabbitMQ
- **Job Scheduling**: Delayed jobs, recurring tasks, cron-like scheduling
- **Reliability**: Job retries, dead letter queues, failure handling
- **Concurrency**: Worker pools, priority queues, rate limiting
- **Monitoring**: Real-time metrics, job tracking, health checks
- **Persistence**: Durable job storage with configurable retention
- **Distributed**: Multi-instance coordination and load balancing

## Installation

<Tabs items={['Go Module', 'Docker', 'Binary']}>
  <Tab value="Go Module">
    ```bash
    go get github.com/forge-framework/forge/v2/extensions/queue
    ```
  </Tab>
  <Tab value="Docker">
    ```dockerfile
    FROM forge/base:v2
    RUN forge extension install queue
    ```
  </Tab>
  <Tab value="Binary">
    ```bash
    forge extension install queue
    ```
  </Tab>
</Tabs>

## Basic Usage

### Simple Job Processing

<Tabs items={['Producer', 'Consumer', 'Job Definition']}>
  <Tab value="Producer">
    ```go
    package main

    import (
        "context"
        "log"
        "time"

        "github.com/forge-framework/forge/v2"
        "github.com/forge-framework/forge/v2/extensions/queue"
    )

    func main() {
        app := forge.NewApp()
        
        // Register Queue extension
        app.RegisterExtension(queue.NewExtension(
            queue.WithRedisBackend("redis://localhost:6379"),
            queue.WithDefaultQueue("default"),
        ))
        
        ctx := context.Background()
        if err := app.Start(ctx); err != nil {
            log.Fatal(err)
        }
        defer app.Stop(ctx)
        
        // Get queue manager
        queueManager := app.GetExtension("queue").(*queue.Extension).GetManager()
        
        // Enqueue simple job
        job := &queue.Job{
            Type:    "send_email",
            Payload: map[string]interface{}{
                "to":      "user@example.com",
                "subject": "Welcome!",
                "body":    "Welcome to our service!",
            },
        }
        
        jobID, err := queueManager.Enqueue(ctx, job)
        if err != nil {
            log.Printf("Failed to enqueue job: %v", err)
            return
        }
        
        log.Printf("Job enqueued with ID: %s", jobID)
        
        // Enqueue delayed job
        delayedJob := &queue.Job{
            Type:    "send_reminder",
            Payload: map[string]interface{}{
                "user_id": 123,
                "message": "Don't forget to complete your profile!",
            },
            Delay: 24 * time.Hour, // Send after 24 hours
        }
        
        delayedJobID, err := queueManager.Enqueue(ctx, delayedJob)
        if err != nil {
            log.Printf("Failed to enqueue delayed job: %v", err)
            return
        }
        
        log.Printf("Delayed job enqueued with ID: %s", delayedJobID)
    }
    ```
  </Tab>
  <Tab value="Consumer">
    ```go
    package main

    import (
        "context"
        "log"
        "time"

        "github.com/forge-framework/forge/v2"
        "github.com/forge-framework/forge/v2/extensions/queue"
    )

    func main() {
        app := forge.NewApp()
        
        // Register Queue extension
        app.RegisterExtension(queue.NewExtension(
            queue.WithRedisBackend("redis://localhost:6379"),
            queue.WithWorkerPool(queue.WorkerPoolConfig{
                Workers:     5,
                Concurrency: 10,
            }),
        ))
        
        ctx := context.Background()
        if err := app.Start(ctx); err != nil {
            log.Fatal(err)
        }
        defer app.Stop(ctx)
        
        // Get queue manager
        queueManager := app.GetExtension("queue").(*queue.Extension).GetManager()
        
        // Register job handlers
        queueManager.RegisterHandler("send_email", handleSendEmail)
        queueManager.RegisterHandler("send_reminder", handleSendReminder)
        queueManager.RegisterHandler("process_image", handleProcessImage)
        
        // Start workers
        if err := queueManager.StartWorkers(ctx); err != nil {
            log.Fatalf("Failed to start workers: %v", err)
        }
        
        log.Println("Queue workers started, processing jobs...")
        
        // Keep the application running
        select {}
    }

    // Job handler for sending emails
    func handleSendEmail(ctx context.Context, job *queue.Job) error {
        to := job.Payload["to"].(string)
        subject := job.Payload["subject"].(string)
        body := job.Payload["body"].(string)
        
        log.Printf("Sending email to %s: %s", to, subject)
        
        // Simulate email sending
        time.Sleep(2 * time.Second)
        
        // Simulate occasional failures for retry testing
        if time.Now().Unix()%10 == 0 {
            return errors.New("email service temporarily unavailable")
        }
        
        log.Printf("Email sent successfully to %s", to)
        return nil
    }

    // Job handler for sending reminders
    func handleSendReminder(ctx context.Context, job *queue.Job) error {
        userID := int(job.Payload["user_id"].(float64))
        message := job.Payload["message"].(string)
        
        log.Printf("Sending reminder to user %d: %s", userID, message)
        
        // Implement reminder logic here
        time.Sleep(1 * time.Second)
        
        log.Printf("Reminder sent to user %d", userID)
        return nil
    }

    // Job handler for image processing
    func handleProcessImage(ctx context.Context, job *queue.Job) error {
        imageURL := job.Payload["image_url"].(string)
        operations := job.Payload["operations"].([]interface{})
        
        log.Printf("Processing image: %s with operations: %v", imageURL, operations)
        
        // Simulate image processing
        time.Sleep(5 * time.Second)
        
        log.Printf("Image processing completed for: %s", imageURL)
        return nil
    }
    ```
  </Tab>
  <Tab value="Job Definition">
    ```go
    package jobs

    import (
        "context"
        "time"

        "github.com/forge-framework/forge/v2/extensions/queue"
    )

    // EmailJob represents an email sending job
    type EmailJob struct {
        To      string `json:"to"`
        Subject string `json:"subject"`
        Body    string `json:"body"`
        From    string `json:"from,omitempty"`
    }

    // NewEmailJob creates a new email job
    func NewEmailJob(to, subject, body string) *queue.Job {
        return &queue.Job{
            Type: "send_email",
            Payload: EmailJob{
                To:      to,
                Subject: subject,
                Body:    body,
            },
            Priority: queue.PriorityNormal,
            MaxRetries: 3,
            Timeout: 30 * time.Second,
        }
    }

    // ImageProcessingJob represents an image processing job
    type ImageProcessingJob struct {
        ImageURL   string   `json:"image_url"`
        Operations []string `json:"operations"`
        OutputPath string   `json:"output_path"`
        UserID     int64    `json:"user_id"`
    }

    // NewImageProcessingJob creates a new image processing job
    func NewImageProcessingJob(imageURL string, operations []string, outputPath string, userID int64) *queue.Job {
        return &queue.Job{
            Type: "process_image",
            Payload: ImageProcessingJob{
                ImageURL:   imageURL,
                Operations: operations,
                OutputPath: outputPath,
                UserID:     userID,
            },
            Priority: queue.PriorityHigh,
            MaxRetries: 2,
            Timeout: 5 * time.Minute,
        }
    }

    // ReportGenerationJob represents a report generation job
    type ReportGenerationJob struct {
        ReportType string                 `json:"report_type"`
        Parameters map[string]interface{} `json:"parameters"`
        UserID     int64                  `json:"user_id"`
        Format     string                 `json:"format"`
    }

    // NewReportGenerationJob creates a new report generation job
    func NewReportGenerationJob(reportType string, params map[string]interface{}, userID int64, format string) *queue.Job {
        return &queue.Job{
            Type: "generate_report",
            Payload: ReportGenerationJob{
                ReportType: reportType,
                Parameters: params,
                UserID:     userID,
                Format:     format,
            },
            Priority: queue.PriorityLow,
            MaxRetries: 1,
            Timeout: 10 * time.Minute,
        }
    }
    ```
  </Tab>
</Tabs>

### Scheduled Jobs

<Tabs items={['Cron Jobs', 'Recurring Tasks', 'One-time Scheduled']}>
  <Tab value="Cron Jobs">
    ```go
    package main

    import (
        "context"
        "log"

        "github.com/forge-framework/forge/v2"
        "github.com/forge-framework/forge/v2/extensions/queue"
    )

    func main() {
        app := forge.NewApp()
        
        app.RegisterExtension(queue.NewExtension(
            queue.WithRedisBackend("redis://localhost:6379"),
            queue.WithScheduler(queue.SchedulerConfig{
                Enabled:  true,
                Timezone: "UTC",
            }),
        ))
        
        ctx := context.Background()
        if err := app.Start(ctx); err != nil {
            log.Fatal(err)
        }
        defer app.Stop(ctx)
        
        queueManager := app.GetExtension("queue").(*queue.Extension).GetManager()
        
        // Schedule daily backup job at 2 AM
        err := queueManager.ScheduleCron(ctx, &queue.CronJob{
            Name:     "daily_backup",
            Schedule: "0 2 * * *", // Every day at 2 AM
            Job: &queue.Job{
                Type: "backup_database",
                Payload: map[string]interface{}{
                    "backup_type": "full",
                    "retention":   "30d",
                },
            },
        })
        if err != nil {
            log.Printf("Failed to schedule backup job: %v", err)
        }
        
        // Schedule weekly report generation
        err = queueManager.ScheduleCron(ctx, &queue.CronJob{
            Name:     "weekly_report",
            Schedule: "0 9 * * 1", // Every Monday at 9 AM
            Job: &queue.Job{
                Type: "generate_report",
                Payload: map[string]interface{}{
                    "report_type": "weekly_summary",
                    "recipients":  []string{"admin@example.com"},
                },
            },
        })
        if err != nil {
            log.Printf("Failed to schedule weekly report: %v", err)
        }
        
        // Schedule hourly cleanup job
        err = queueManager.ScheduleCron(ctx, &queue.CronJob{
            Name:     "cleanup_temp_files",
            Schedule: "0 * * * *", // Every hour
            Job: &queue.Job{
                Type: "cleanup_files",
                Payload: map[string]interface{}{
                    "directory": "/tmp",
                    "max_age":   "1h",
                },
            },
        })
        if err != nil {
            log.Printf("Failed to schedule cleanup job: %v", err)
        }
        
        log.Println("Scheduled jobs configured successfully")
    }
    ```
  </Tab>
  <Tab value="Recurring Tasks">
    ```go
    package main

    import (
        "context"
        "log"
        "time"

        "github.com/forge-framework/forge/v2"
        "github.com/forge-framework/forge/v2/extensions/queue"
    )

    func main() {
        app := forge.NewApp()
        
        app.RegisterExtension(queue.NewExtension(
            queue.WithRedisBackend("redis://localhost:6379"),
        ))
        
        ctx := context.Background()
        if err := app.Start(ctx); err != nil {
            log.Fatal(err)
        }
        defer app.Stop(ctx)
        
        queueManager := app.GetExtension("queue").(*queue.Extension).GetManager()
        
        // Schedule recurring health check every 5 minutes
        err := queueManager.ScheduleRecurring(ctx, &queue.RecurringJob{
            Name:     "health_check",
            Interval: 5 * time.Minute,
            Job: &queue.Job{
                Type: "health_check",
                Payload: map[string]interface{}{
                    "services": []string{"database", "redis", "api"},
                },
            },
        })
        if err != nil {
            log.Printf("Failed to schedule health check: %v", err)
        }
        
        // Schedule data synchronization every 30 minutes
        err = queueManager.ScheduleRecurring(ctx, &queue.RecurringJob{
            Name:     "data_sync",
            Interval: 30 * time.Minute,
            Job: &queue.Job{
                Type: "sync_data",
                Payload: map[string]interface{}{
                    "source": "external_api",
                    "target": "local_database",
                },
            },
        })
        if err != nil {
            log.Printf("Failed to schedule data sync: %v", err)
        }
        
        // Schedule metrics collection every minute
        err = queueManager.ScheduleRecurring(ctx, &queue.RecurringJob{
            Name:     "collect_metrics",
            Interval: 1 * time.Minute,
            Job: &queue.Job{
                Type: "collect_metrics",
                Payload: map[string]interface{}{
                    "metrics": []string{"cpu", "memory", "disk", "network"},
                },
            },
        })
        if err != nil {
            log.Printf("Failed to schedule metrics collection: %v", err)
        }
        
        log.Println("Recurring jobs scheduled successfully")
    }
    ```
  </Tab>
  <Tab value="One-time Scheduled">
    ```go
    package main

    import (
        "context"
        "log"
        "time"

        "github.com/forge-framework/forge/v2"
        "github.com/forge-framework/forge/v2/extensions/queue"
    )

    func main() {
        app := forge.NewApp()
        
        app.RegisterExtension(queue.NewExtension(
            queue.WithRedisBackend("redis://localhost:6379"),
        ))
        
        ctx := context.Background()
        if err := app.Start(ctx); err != nil {
            log.Fatal(err)
        }
        defer app.Stop(ctx)
        
        queueManager := app.GetExtension("queue").(*queue.Extension).GetManager()
        
        // Schedule one-time job for specific time
        scheduledTime := time.Now().Add(1 * time.Hour)
        err := queueManager.ScheduleAt(ctx, scheduledTime, &queue.Job{
            Type: "send_newsletter",
            Payload: map[string]interface{}{
                "campaign_id": "newsletter_2024_01",
                "recipients":  1000,
            },
        })
        if err != nil {
            log.Printf("Failed to schedule newsletter: %v", err)
        }
        
        // Schedule maintenance window
        maintenanceTime := time.Date(2024, 1, 15, 2, 0, 0, 0, time.UTC)
        err = queueManager.ScheduleAt(ctx, maintenanceTime, &queue.Job{
            Type: "maintenance_mode",
            Payload: map[string]interface{}{
                "action":   "enable",
                "duration": "2h",
                "message":  "Scheduled maintenance in progress",
            },
        })
        if err != nil {
            log.Printf("Failed to schedule maintenance: %v", err)
        }
        
        // Schedule user trial expiration reminder
        reminderTime := time.Now().Add(7 * 24 * time.Hour) // 7 days from now
        err = queueManager.ScheduleAt(ctx, reminderTime, &queue.Job{
            Type: "trial_expiration_reminder",
            Payload: map[string]interface{}{
                "user_id":     12345,
                "trial_ends":  reminderTime.Add(7 * 24 * time.Hour),
                "upgrade_url": "https://example.com/upgrade",
            },
        })
        if err != nil {
            log.Printf("Failed to schedule trial reminder: %v", err)
        }
        
        log.Println("One-time scheduled jobs created successfully")
    }
    ```
  </Tab>
</Tabs>

### Priority Queues

<Tabs items={['Priority Levels', 'Queue Management', 'Load Balancing']}>
  <Tab value="Priority Levels">
    ```go
    package main

    import (
        "context"
        "log"

        "github.com/forge-framework/forge/v2"
        "github.com/forge-framework/forge/v2/extensions/queue"
    )

    func main() {
        app := forge.NewApp()
        
        app.RegisterExtension(queue.NewExtension(
            queue.WithRedisBackend("redis://localhost:6379"),
            queue.WithPriorityQueues([]string{
                "critical",  // Highest priority
                "high",
                "normal",
                "low",       // Lowest priority
            }),
        ))
        
        ctx := context.Background()
        if err := app.Start(ctx); err != nil {
            log.Fatal(err)
        }
        defer app.Stop(ctx)
        
        queueManager := app.GetExtension("queue").(*queue.Extension).GetManager()
        
        // Critical priority - system alerts
        criticalJob := &queue.Job{
            Type:     "system_alert",
            Priority: queue.PriorityCritical,
            Payload: map[string]interface{}{
                "alert_type": "database_down",
                "severity":   "critical",
                "message":    "Database connection lost",
            },
        }
        
        jobID, err := queueManager.EnqueueToPriority(ctx, "critical", criticalJob)
        if err != nil {
            log.Printf("Failed to enqueue critical job: %v", err)
        } else {
            log.Printf("Critical job enqueued: %s", jobID)
        }
        
        // High priority - user-facing operations
        highJob := &queue.Job{
            Type:     "process_payment",
            Priority: queue.PriorityHigh,
            Payload: map[string]interface{}{
                "payment_id": "pay_123456",
                "amount":     99.99,
                "currency":   "USD",
            },
        }
        
        jobID, err = queueManager.EnqueueToPriority(ctx, "high", highJob)
        if err != nil {
            log.Printf("Failed to enqueue high priority job: %v", err)
        } else {
            log.Printf("High priority job enqueued: %s", jobID)
        }
        
        // Normal priority - regular operations
        normalJob := &queue.Job{
            Type:     "send_email",
            Priority: queue.PriorityNormal,
            Payload: map[string]interface{}{
                "to":      "user@example.com",
                "subject": "Order confirmation",
            },
        }
        
        jobID, err = queueManager.EnqueueToPriority(ctx, "normal", normalJob)
        if err != nil {
            log.Printf("Failed to enqueue normal job: %v", err)
        } else {
            log.Printf("Normal priority job enqueued: %s", jobID)
        }
        
        // Low priority - background tasks
        lowJob := &queue.Job{
            Type:     "generate_analytics",
            Priority: queue.PriorityLow,
            Payload: map[string]interface{}{
                "date_range": "last_30_days",
                "report_id":  "analytics_001",
            },
        }
        
        jobID, err = queueManager.EnqueueToPriority(ctx, "low", lowJob)
        if err != nil {
            log.Printf("Failed to enqueue low priority job: %v", err)
        } else {
            log.Printf("Low priority job enqueued: %s", jobID)
        }
    }
    ```
  </Tab>
  <Tab value="Queue Management">
    ```go
    package main

    import (
        "context"
        "log"

        "github.com/forge-framework/forge/v2"
        "github.com/forge-framework/forge/v2/extensions/queue"
    )

    func setupQueueManagement() {
        app := forge.NewApp()
        
        app.RegisterExtension(queue.NewExtension(
            queue.WithRedisBackend("redis://localhost:6379"),
            queue.WithQueues(map[string]queue.QueueConfig{
                "critical": {
                    MaxWorkers:   10,
                    MaxRetries:   5,
                    RetryDelay:   time.Second,
                    Timeout:      30 * time.Second,
                    RateLimit:    100, // jobs per second
                },
                "high": {
                    MaxWorkers:   8,
                    MaxRetries:   3,
                    RetryDelay:   2 * time.Second,
                    Timeout:      60 * time.Second,
                    RateLimit:    50,
                },
                "normal": {
                    MaxWorkers:   5,
                    MaxRetries:   3,
                    RetryDelay:   5 * time.Second,
                    Timeout:      2 * time.Minute,
                    RateLimit:    20,
                },
                "low": {
                    MaxWorkers:   2,
                    MaxRetries:   1,
                    RetryDelay:   10 * time.Second,
                    Timeout:      5 * time.Minute,
                    RateLimit:    5,
                },
            }),
        ))
        
        ctx := context.Background()
        if err := app.Start(ctx); err != nil {
            log.Fatal(err)
        }
        defer app.Stop(ctx)
        
        queueManager := app.GetExtension("queue").(*queue.Extension).GetManager()
        
        // Monitor queue sizes
        go func() {
            ticker := time.NewTicker(30 * time.Second)
            defer ticker.Stop()
            
            for {
                select {
                case <-ticker.C:
                    stats := queueManager.GetQueueStats(ctx)
                    for queueName, stat := range stats {
                        log.Printf("Queue %s: pending=%d, processing=%d, completed=%d, failed=%d",
                            queueName, stat.Pending, stat.Processing, stat.Completed, stat.Failed)
                        
                        // Auto-scale workers based on queue size
                        if stat.Pending > 100 && queueName != "low" {
                            queueManager.ScaleWorkers(ctx, queueName, stat.Workers+2)
                        } else if stat.Pending < 10 && stat.Workers > 1 {
                            queueManager.ScaleWorkers(ctx, queueName, stat.Workers-1)
                        }
                    }
                case <-ctx.Done():
                    return
                }
            }
        }()
        
        // Pause/resume queues based on system load
        systemLoad := getSystemLoad()
        if systemLoad > 0.8 {
            queueManager.PauseQueue(ctx, "low")
            log.Println("Paused low priority queue due to high system load")
        } else {
            queueManager.ResumeQueue(ctx, "low")
            log.Println("Resumed low priority queue")
        }
    }

    func getSystemLoad() float64 {
        // Implement system load monitoring
        return 0.5 // Placeholder
    }
    ```
  </Tab>
  <Tab value="Load Balancing">
    ```go
    package main

    import (
        "context"
        "log"
        "sync"

        "github.com/forge-framework/forge/v2"
        "github.com/forge-framework/forge/v2/extensions/queue"
    )

    // LoadBalancer distributes jobs across multiple queues
    type LoadBalancer struct {
        queues    []string
        current   int
        mu        sync.Mutex
        queueLoad map[string]int
    }

    func NewLoadBalancer(queues []string) *LoadBalancer {
        return &LoadBalancer{
            queues:    queues,
            queueLoad: make(map[string]int),
        }
    }

    // GetNextQueue returns the queue with the least load
    func (lb *LoadBalancer) GetNextQueue() string {
        lb.mu.Lock()
        defer lb.mu.Unlock()
        
        minLoad := int(^uint(0) >> 1) // Max int
        selectedQueue := lb.queues[0]
        
        for _, queue := range lb.queues {
            load := lb.queueLoad[queue]
            if load < minLoad {
                minLoad = load
                selectedQueue = queue
            }
        }
        
        lb.queueLoad[selectedQueue]++
        return selectedQueue
    }

    // ReleaseQueue decrements the load for a queue
    func (lb *LoadBalancer) ReleaseQueue(queue string) {
        lb.mu.Lock()
        defer lb.mu.Unlock()
        
        if lb.queueLoad[queue] > 0 {
            lb.queueLoad[queue]--
        }
    }

    func setupLoadBalancing() {
        app := forge.NewApp()
        
        app.RegisterExtension(queue.NewExtension(
            queue.WithRedisBackend("redis://localhost:6379"),
            queue.WithQueues(map[string]queue.QueueConfig{
                "worker-1": {MaxWorkers: 5, RateLimit: 50},
                "worker-2": {MaxWorkers: 5, RateLimit: 50},
                "worker-3": {MaxWorkers: 5, RateLimit: 50},
                "worker-4": {MaxWorkers: 5, RateLimit: 50},
            }),
        ))
        
        ctx := context.Background()
        if err := app.Start(ctx); err != nil {
            log.Fatal(err)
        }
        defer app.Stop(ctx)
        
        queueManager := app.GetExtension("queue").(*queue.Extension).GetManager()
        loadBalancer := NewLoadBalancer([]string{"worker-1", "worker-2", "worker-3", "worker-4"})
        
        // Distribute jobs across workers
        for i := 0; i < 100; i++ {
            selectedQueue := loadBalancer.GetNextQueue()
            
            job := &queue.Job{
                Type: "process_data",
                Payload: map[string]interface{}{
                    "data_id": i,
                    "batch":   i / 10,
                },
            }
            
            jobID, err := queueManager.EnqueueToPriority(ctx, selectedQueue, job)
            if err != nil {
                log.Printf("Failed to enqueue job to %s: %v", selectedQueue, err)
                loadBalancer.ReleaseQueue(selectedQueue)
                continue
            }
            
            log.Printf("Job %s enqueued to %s", jobID, selectedQueue)
            
            // Simulate job completion callback
            go func(queue string) {
                // Wait for job completion (simplified)
                time.Sleep(time.Duration(rand.Intn(5)) * time.Second)
                loadBalancer.ReleaseQueue(queue)
            }(selectedQueue)
        }
    }
    ```
  </Tab>
</Tabs>

### Batch Processing

<Tabs items={['Batch Jobs', 'Parallel Processing', 'Result Aggregation']}>
  <Tab value="Batch Jobs">
    ```go
    package main

    import (
        "context"
        "log"

        "github.com/forge-framework/forge/v2"
        "github.com/forge-framework/forge/v2/extensions/queue"
    )

    func main() {
        app := forge.NewApp()
        
        app.RegisterExtension(queue.NewExtension(
            queue.WithRedisBackend("redis://localhost:6379"),
            queue.WithBatchProcessing(queue.BatchConfig{
                Enabled:     true,
                BatchSize:   50,
                MaxWaitTime: 30 * time.Second,
            }),
        ))
        
        ctx := context.Background()
        if err := app.Start(ctx); err != nil {
            log.Fatal(err)
        }
        defer app.Stop(ctx)
        
        queueManager := app.GetExtension("queue").(*queue.Extension).GetManager()
        
        // Register batch job handler
        queueManager.RegisterBatchHandler("process_emails", handleEmailBatch)
        queueManager.RegisterBatchHandler("process_images", handleImageBatch)
        
        // Enqueue individual jobs that will be batched
        for i := 0; i < 100; i++ {
            emailJob := &queue.Job{
                Type: "process_emails",
                Payload: map[string]interface{}{
                    "email_id": i,
                    "to":       fmt.Sprintf("user%d@example.com", i),
                    "template": "welcome",
                },
                BatchKey: "email_batch", // Jobs with same batch key are grouped
            }
            
            _, err := queueManager.Enqueue(ctx, emailJob)
            if err != nil {
                log.Printf("Failed to enqueue email job %d: %v", i, err)
            }
        }
        
        // Enqueue image processing jobs
        for i := 0; i < 25; i++ {
            imageJob := &queue.Job{
                Type: "process_images",
                Payload: map[string]interface{}{
                    "image_id": i,
                    "url":      fmt.Sprintf("https://example.com/image%d.jpg", i),
                    "resize":   []int{800, 600},
                },
                BatchKey: "image_batch",
            }
            
            _, err := queueManager.Enqueue(ctx, imageJob)
            if err != nil {
                log.Printf("Failed to enqueue image job %d: %v", i, err)
            }
        }
        
        log.Println("Batch jobs enqueued successfully")
    }

    // Handle batch of email jobs
    func handleEmailBatch(ctx context.Context, jobs []*queue.Job) error {
        log.Printf("Processing batch of %d email jobs", len(jobs))
        
        // Prepare batch email sending
        var emails []EmailData
        for _, job := range jobs {
            emails = append(emails, EmailData{
                ID:       int(job.Payload["email_id"].(float64)),
                To:       job.Payload["to"].(string),
                Template: job.Payload["template"].(string),
            })
        }
        
        // Send emails in batch
        if err := sendEmailBatch(emails); err != nil {
            log.Printf("Failed to send email batch: %v", err)
            return err
        }
        
        log.Printf("Successfully processed %d emails in batch", len(emails))
        return nil
    }

    // Handle batch of image processing jobs
    func handleImageBatch(ctx context.Context, jobs []*queue.Job) error {
        log.Printf("Processing batch of %d image jobs", len(jobs))
        
        // Process images in parallel within the batch
        var wg sync.WaitGroup
        errChan := make(chan error, len(jobs))
        
        for _, job := range jobs {
            wg.Add(1)
            go func(j *queue.Job) {
                defer wg.Done()
                
                imageID := int(j.Payload["image_id"].(float64))
                imageURL := j.Payload["url"].(string)
                resize := j.Payload["resize"].([]interface{})
                
                if err := processImage(imageID, imageURL, resize); err != nil {
                    errChan <- fmt.Errorf("failed to process image %d: %v", imageID, err)
                    return
                }
                
                log.Printf("Processed image %d", imageID)
            }(job)
        }
        
        wg.Wait()
        close(errChan)
        
        // Check for errors
        var errors []error
        for err := range errChan {
            errors = append(errors, err)
        }
        
        if len(errors) > 0 {
            return fmt.Errorf("batch processing failed with %d errors: %v", len(errors), errors[0])
        }
        
        log.Printf("Successfully processed %d images in batch", len(jobs))
        return nil
    }

    type EmailData struct {
        ID       int
        To       string
        Template string
    }

    func sendEmailBatch(emails []EmailData) error {
        // Implement batch email sending
        log.Printf("Sending %d emails in batch", len(emails))
        time.Sleep(2 * time.Second) // Simulate batch processing
        return nil
    }

    func processImage(id int, url string, resize []interface{}) error {
        // Implement image processing
        log.Printf("Processing image %d: %s", id, url)
        time.Sleep(1 * time.Second) // Simulate processing
        return nil
    }
    ```
  </Tab>
  <Tab value="Parallel Processing">
    ```go
    package main

    import (
        "context"
        "log"
        "sync"

        "github.com/forge-framework/forge/v2"
        "github.com/forge-framework/forge/v2/extensions/queue"
    )

    func setupParallelProcessing() {
        app := forge.NewApp()
        
        app.RegisterExtension(queue.NewExtension(
            queue.WithRedisBackend("redis://localhost:6379"),
            queue.WithParallelProcessing(queue.ParallelConfig{
                MaxConcurrency: 10,
                WorkerPools: map[string]int{
                    "cpu_intensive": 4,
                    "io_intensive":  8,
                    "mixed":         6,
                },
            }),
        ))
        
        ctx := context.Background()
        if err := app.Start(ctx); err != nil {
            log.Fatal(err)
        }
        defer app.Stop(ctx)
        
        queueManager := app.GetExtension("queue").(*queue.Extension).GetManager()
        
        // Register parallel job handlers
        queueManager.RegisterParallelHandler("data_processing", handleDataProcessingParallel)
        queueManager.RegisterParallelHandler("file_conversion", handleFileConversionParallel)
        
        // Create a large dataset processing job
        datasetJob := &queue.Job{
            Type: "data_processing",
            Payload: map[string]interface{}{
                "dataset_id":   "dataset_001",
                "total_records": 10000,
                "chunk_size":   1000,
                "operation":    "transform",
            },
            ParallelConfig: &queue.ParallelJobConfig{
                MaxWorkers:   8,
                ChunkSize:    1000,
                Timeout:      10 * time.Minute,
            },
        }
        
        jobID, err := queueManager.EnqueueParallel(ctx, datasetJob)
        if err != nil {
            log.Printf("Failed to enqueue parallel job: %v", err)
        } else {
            log.Printf("Parallel data processing job enqueued: %s", jobID)
        }
        
        // Create file conversion jobs
        files := []string{
            "video1.mp4", "video2.mp4", "video3.mp4",
            "audio1.wav", "audio2.wav", "audio3.wav",
        }
        
        conversionJob := &queue.Job{
            Type: "file_conversion",
            Payload: map[string]interface{}{
                "files":        files,
                "output_format": "compressed",
                "quality":      "high",
            },
            ParallelConfig: &queue.ParallelJobConfig{
                MaxWorkers: 4,
                ChunkSize:  2,
                Timeout:    30 * time.Minute,
            },
        }
        
        jobID, err = queueManager.EnqueueParallel(ctx, conversionJob)
        if err != nil {
            log.Printf("Failed to enqueue conversion job: %v", err)
        } else {
            log.Printf("Parallel file conversion job enqueued: %s", jobID)
        }
    }

    // Handle data processing in parallel
    func handleDataProcessingParallel(ctx context.Context, job *queue.Job) error {
        datasetID := job.Payload["dataset_id"].(string)
        totalRecords := int(job.Payload["total_records"].(float64))
        chunkSize := int(job.Payload["chunk_size"].(float64))
        operation := job.Payload["operation"].(string)
        
        log.Printf("Starting parallel processing of dataset %s with %d records", datasetID, totalRecords)
        
        // Calculate number of chunks
        numChunks := (totalRecords + chunkSize - 1) / chunkSize
        
        // Create worker pool
        var wg sync.WaitGroup
        semaphore := make(chan struct{}, job.ParallelConfig.MaxWorkers)
        errChan := make(chan error, numChunks)
        
        // Process chunks in parallel
        for i := 0; i < numChunks; i++ {
            wg.Add(1)
            go func(chunkIndex int) {
                defer wg.Done()
                
                // Acquire semaphore
                semaphore <- struct{}{}
                defer func() { <-semaphore }()
                
                start := chunkIndex * chunkSize
                end := start + chunkSize
                if end > totalRecords {
                    end = totalRecords
                }
                
                if err := processDataChunk(ctx, datasetID, start, end, operation); err != nil {
                    errChan <- fmt.Errorf("chunk %d failed: %v", chunkIndex, err)
                    return
                }
                
                log.Printf("Processed chunk %d (%d-%d) of dataset %s", chunkIndex, start, end, datasetID)
            }(i)
        }
        
        wg.Wait()
        close(errChan)
        
        // Check for errors
        var errors []error
        for err := range errChan {
            errors = append(errors, err)
        }
        
        if len(errors) > 0 {
            return fmt.Errorf("parallel processing failed with %d errors: %v", len(errors), errors[0])
        }
        
        log.Printf("Successfully processed dataset %s with %d records", datasetID, totalRecords)
        return nil
    }

    // Handle file conversion in parallel
    func handleFileConversionParallel(ctx context.Context, job *queue.Job) error {
        files := job.Payload["files"].([]interface{})
        outputFormat := job.Payload["output_format"].(string)
        quality := job.Payload["quality"].(string)
        
        log.Printf("Starting parallel conversion of %d files", len(files))
        
        // Create worker pool
        var wg sync.WaitGroup
        semaphore := make(chan struct{}, job.ParallelConfig.MaxWorkers)
        errChan := make(chan error, len(files))
        
        // Convert files in parallel
        for i, file := range files {
            wg.Add(1)
            go func(index int, filename string) {
                defer wg.Done()
                
                // Acquire semaphore
                semaphore <- struct{}{}
                defer func() { <-semaphore }()
                
                if err := convertFile(ctx, filename, outputFormat, quality); err != nil {
                    errChan <- fmt.Errorf("file %s conversion failed: %v", filename, err)
                    return
                }
                
                log.Printf("Converted file %s (%d/%d)", filename, index+1, len(files))
            }(i, file.(string))
        }
        
        wg.Wait()
        close(errChan)
        
        // Check for errors
        var errors []error
        for err := range errChan {
            errors = append(errors, err)
        }
        
        if len(errors) > 0 {
            return fmt.Errorf("parallel conversion failed with %d errors: %v", len(errors), errors[0])
        }
        
        log.Printf("Successfully converted %d files", len(files))
        return nil
    }

    func processDataChunk(ctx context.Context, datasetID string, start, end int, operation string) error {
        // Implement data chunk processing
        log.Printf("Processing data chunk %s[%d:%d] with operation %s", datasetID, start, end, operation)
        time.Sleep(2 * time.Second) // Simulate processing
        return nil
    }

    func convertFile(ctx context.Context, filename, format, quality string) error {
        // Implement file conversion
        log.Printf("Converting file %s to %s format with %s quality", filename, format, quality)
        time.Sleep(5 * time.Second) // Simulate conversion
        return nil
    }
    ```
  </Tab>
  <Tab value="Result Aggregation">
    ```go
    package main

    import (
        "context"
        "log"
        "sync"

        "github.com/forge-framework/forge/v2"
        "github.com/forge-framework/forge/v2/extensions/queue"
    )

    // ResultAggregator collects and combines results from parallel jobs
    type ResultAggregator struct {
        results map[string]interface{}
        mu      sync.RWMutex
        done    chan struct{}
        total   int
        count   int
    }

    func NewResultAggregator(total int) *ResultAggregator {
        return &ResultAggregator{
            results: make(map[string]interface{}),
            done:    make(chan struct{}),
            total:   total,
        }
    }

    func (ra *ResultAggregator) AddResult(key string, result interface{}) {
        ra.mu.Lock()
        defer ra.mu.Unlock()
        
        ra.results[key] = result
        ra.count++
        
        if ra.count >= ra.total {
            close(ra.done)
        }
    }

    func (ra *ResultAggregator) GetResults() map[string]interface{} {
        ra.mu.RLock()
        defer ra.mu.RUnlock()
        
        results := make(map[string]interface{})
        for k, v := range ra.results {
            results[k] = v
        }
        return results
    }

    func (ra *ResultAggregator) Wait() <-chan struct{} {
        return ra.done
    }

    func setupResultAggregation() {
        app := forge.NewApp()
        
        app.RegisterExtension(queue.NewExtension(
            queue.WithRedisBackend("redis://localhost:6379"),
        ))
        
        ctx := context.Background()
        if err := app.Start(ctx); err != nil {
            log.Fatal(err)
        }
        defer app.Stop(ctx)
        
        queueManager := app.GetExtension("queue").(*queue.Extension).GetManager()
        
        // Create aggregator for analytics job
        aggregator := NewResultAggregator(5) // Expecting 5 results
        
        // Register handler with result aggregation
        queueManager.RegisterHandler("analytics_chunk", func(ctx context.Context, job *queue.Job) error {
            return handleAnalyticsChunk(ctx, job, aggregator)
        })
        
        // Enqueue analytics jobs for different data segments
        segments := []string{"users", "orders", "products", "reviews", "payments"}
        
        for _, segment := range segments {
            analyticsJob := &queue.Job{
                Type: "analytics_chunk",
                Payload: map[string]interface{}{
                    "segment":    segment,
                    "date_range": "last_30_days",
                    "metrics":    []string{"count", "sum", "avg", "max", "min"},
                },
            }
            
            _, err := queueManager.Enqueue(ctx, analyticsJob)
            if err != nil {
                log.Printf("Failed to enqueue analytics job for %s: %v", segment, err)
            }
        }
        
        // Wait for all results and aggregate
        go func() {
            <-aggregator.Wait()
            
            finalResults := aggregator.GetResults()
            aggregatedReport := aggregateAnalyticsResults(finalResults)
            
            log.Printf("Analytics aggregation completed: %+v", aggregatedReport)
            
            // Store or send the final report
            storeAnalyticsReport(aggregatedReport)
        }()
    }

    func handleAnalyticsChunk(ctx context.Context, job *queue.Job, aggregator *ResultAggregator) error {
        segment := job.Payload["segment"].(string)
        dateRange := job.Payload["date_range"].(string)
        metrics := job.Payload["metrics"].([]interface{})
        
        log.Printf("Processing analytics for segment: %s", segment)
        
        // Simulate analytics processing
        result := map[string]interface{}{
            "segment":    segment,
            "date_range": dateRange,
            "count":      rand.Intn(10000),
            "sum":        rand.Float64() * 100000,
            "avg":        rand.Float64() * 100,
            "max":        rand.Float64() * 1000,
            "min":        rand.Float64() * 10,
            "processed_at": time.Now(),
        }
        
        // Simulate processing time
        time.Sleep(time.Duration(rand.Intn(5)) * time.Second)
        
        // Add result to aggregator
        aggregator.AddResult(segment, result)
        
        log.Printf("Analytics completed for segment: %s", segment)
        return nil
    }

    func aggregateAnalyticsResults(results map[string]interface{}) map[string]interface{} {
        aggregated := map[string]interface{}{
            "total_count": 0,
            "total_sum":   0.0,
            "segments":    make(map[string]interface{}),
            "generated_at": time.Now(),
        }
        
        totalCount := 0
        totalSum := 0.0
        
        for segment, result := range results {
            segmentData := result.(map[string]interface{})
            
            count := segmentData["count"].(int)
            sum := segmentData["sum"].(float64)
            
            totalCount += count
            totalSum += sum
            
            aggregated["segments"].(map[string]interface{})[segment] = segmentData
        }
        
        aggregated["total_count"] = totalCount
        aggregated["total_sum"] = totalSum
        aggregated["overall_avg"] = totalSum / float64(totalCount)
        
        return aggregated
    }

    func storeAnalyticsReport(report map[string]interface{}) {
        // Implement report storage
        log.Printf("Storing analytics report: %+v", report)
    }
    ```
  </Tab>
</Tabs>

## Configuration

### YAML Configuration

```yaml
# config/queue.yaml
queue:
  # Backend configuration
  backend:
    type: "redis"  # redis, memory, database, sqs, rabbitmq
    url: "redis://localhost:6379"
    database: 0
    password: ""
    pool_size: 10
    
  # Default queue settings
  default_queue: "default"
  
  # Queue definitions
  queues:
    critical:
      max_workers: 10
      max_retries: 5
      retry_delay: "1s"
      timeout: "30s"
      rate_limit: 100
      dead_letter_queue: "critical_dlq"
      
    high:
      max_workers: 8
      max_retries: 3
      retry_delay: "2s"
      timeout: "1m"
      rate_limit: 50
      dead_letter_queue: "high_dlq"
      
    normal:
      max_workers: 5
      max_retries: 3
      retry_delay: "5s"
      timeout: "2m"
      rate_limit: 20
      dead_letter_queue: "normal_dlq"
      
    low:
      max_workers: 2
      max_retries: 1
      retry_delay: "10s"
      timeout: "5m"
      rate_limit: 5
      dead_letter_queue: "low_dlq"
  
  # Worker pool configuration
  worker_pool:
    workers: 5
    concurrency: 10
    shutdown_timeout: "30s"
    health_check_interval: "10s"
    
  # Scheduler configuration
  scheduler:
    enabled: true
    timezone: "UTC"
    max_concurrent_jobs: 100
    cleanup_interval: "1h"
    
  # Batch processing
  batch:
    enabled: true
    batch_size: 50
    max_wait_time: "30s"
    max_batch_size: 1000
    
  # Monitoring and metrics
  monitoring:
    enabled: true
    metrics_interval: "30s"
    health_check_endpoint: "/health/queue"
    prometheus_metrics: true
    
  # Persistence settings
  persistence:
    enabled: true
    retention_period: "7d"
    cleanup_interval: "1h"
    backup_enabled: false
    
  # Security settings
  security:
    encryption_enabled: false
    encryption_key: ""
    auth_required: false
    allowed_job_types: []
    
  # Advanced features
  features:
    priority_queues: true
    delayed_jobs: true
    recurring_jobs: true
    job_chaining: true
    result_storage: true
    distributed_locks: true
```

### Environment Variables

```bash
# Backend configuration
QUEUE_BACKEND_TYPE=redis
QUEUE_BACKEND_URL=redis://localhost:6379
QUEUE_BACKEND_DATABASE=0
QUEUE_BACKEND_PASSWORD=
QUEUE_BACKEND_POOL_SIZE=10

# Queue settings
QUEUE_DEFAULT_QUEUE=default
QUEUE_MAX_WORKERS=5
QUEUE_MAX_RETRIES=3
QUEUE_RETRY_DELAY=5s
QUEUE_TIMEOUT=2m
QUEUE_RATE_LIMIT=20

# Worker pool
QUEUE_WORKER_POOL_WORKERS=5
QUEUE_WORKER_POOL_CONCURRENCY=10
QUEUE_WORKER_POOL_SHUTDOWN_TIMEOUT=30s

# Scheduler
QUEUE_SCHEDULER_ENABLED=true
QUEUE_SCHEDULER_TIMEZONE=UTC
QUEUE_SCHEDULER_MAX_CONCURRENT_JOBS=100

# Batch processing
QUEUE_BATCH_ENABLED=true
QUEUE_BATCH_SIZE=50
QUEUE_BATCH_MAX_WAIT_TIME=30s

# Monitoring
QUEUE_MONITORING_ENABLED=true
QUEUE_MONITORING_METRICS_INTERVAL=30s
QUEUE_MONITORING_PROMETHEUS_METRICS=true

# Persistence
QUEUE_PERSISTENCE_ENABLED=true
QUEUE_PERSISTENCE_RETENTION_PERIOD=7d
QUEUE_PERSISTENCE_CLEANUP_INTERVAL=1h

# Security
QUEUE_SECURITY_ENCRYPTION_ENABLED=false
QUEUE_SECURITY_AUTH_REQUIRED=false
```

### Functional Options

```go
package main

import (
    "time"
    
    "github.com/forge-framework/forge/v2/extensions/queue"
)

func configureQueue() queue.Extension {
    return queue.NewExtension(
        // Backend configuration
        queue.WithRedisBackend("redis://localhost:6379"),
        queue.WithRedisPool(10),
        queue.WithRedisDatabase(0),
        
        // Alternative backends
        // queue.WithMemoryBackend(),
        // queue.WithDatabaseBackend("postgres://..."),
        // queue.WithSQSBackend("us-east-1", "queue-name"),
        // queue.WithRabbitMQBackend("amqp://localhost:5672"),
        
        // Queue configuration
        queue.WithDefaultQueue("default"),
        queue.WithQueues(map[string]queue.QueueConfig{
            "critical": {
                MaxWorkers:      10,
                MaxRetries:      5,
                RetryDelay:      time.Second,
                Timeout:         30 * time.Second,
                RateLimit:       100,
                DeadLetterQueue: "critical_dlq",
            },
            "high": {
                MaxWorkers:      8,
                MaxRetries:      3,
                RetryDelay:      2 * time.Second,
                Timeout:         time.Minute,
                RateLimit:       50,
                DeadLetterQueue: "high_dlq",
            },
        }),
        
        // Worker pool configuration
        queue.WithWorkerPool(queue.WorkerPoolConfig{
            Workers:             5,
            Concurrency:         10,
            ShutdownTimeout:     30 * time.Second,
            HealthCheckInterval: 10 * time.Second,
        }),
        
        // Scheduler configuration
        queue.WithScheduler(queue.SchedulerConfig{
            Enabled:           true,
            Timezone:          "UTC",
            MaxConcurrentJobs: 100,
            CleanupInterval:   time.Hour,
        }),
        
        // Batch processing
        queue.WithBatchProcessing(queue.BatchConfig{
            Enabled:      true,
            BatchSize:    50,
            MaxWaitTime:  30 * time.Second,
            MaxBatchSize: 1000,
        }),
        
        // Monitoring
        queue.WithMonitoring(queue.MonitoringConfig{
            Enabled:           true,
            MetricsInterval:   30 * time.Second,
            HealthCheckPath:   "/health/queue",
            PrometheusMetrics: true,
        }),
        
        // Persistence
        queue.WithPersistence(queue.PersistenceConfig{
            Enabled:         true,
            RetentionPeriod: 7 * 24 * time.Hour,
            CleanupInterval: time.Hour,
            BackupEnabled:   false,
        }),
        
        // Security
        queue.WithSecurity(queue.SecurityConfig{
            EncryptionEnabled: false,
            AuthRequired:      false,
            AllowedJobTypes:   []string{},
        }),
        
        // Advanced features
        queue.WithPriorityQueues(true),
        queue.WithDelayedJobs(true),
        queue.WithRecurringJobs(true),
        queue.WithJobChaining(true),
        queue.WithResultStorage(true),
        queue.WithDistributedLocks(true),
    )
}
```

## Queue Manager Interface

```go
package queue

import (
    "context"
    "time"
)

// QueueManager provides the main interface for queue operations
type QueueManager interface {
    // Job operations
    Enqueue(ctx context.Context, job *Job) (string, error)
    EnqueueToPriority(ctx context.Context, queue string, job *Job) (string, error)
    EnqueueParallel(ctx context.Context, job *Job) (string, error)
    EnqueueBatch(ctx context.Context, jobs []*Job) ([]string, error)
    
    // Job scheduling
    ScheduleAt(ctx context.Context, when time.Time, job *Job) (string, error)
    ScheduleIn(ctx context.Context, delay time.Duration, job *Job) (string, error)
    ScheduleCron(ctx context.Context, cronJob *CronJob) error
    ScheduleRecurring(ctx context.Context, recurringJob *RecurringJob) error
    
    // Job management
    GetJob(ctx context.Context, jobID string) (*Job, error)
    CancelJob(ctx context.Context, jobID string) error
    RetryJob(ctx context.Context, jobID string) error
    DeleteJob(ctx context.Context, jobID string) error
    
    // Handler registration
    RegisterHandler(jobType string, handler JobHandler) error
    RegisterBatchHandler(jobType string, handler BatchJobHandler) error
    RegisterParallelHandler(jobType string, handler ParallelJobHandler) error
    
    // Worker management
    StartWorkers(ctx context.Context) error
    StopWorkers(ctx context.Context) error
    ScaleWorkers(ctx context.Context, queue string, count int) error
    
    // Queue management
    PauseQueue(ctx context.Context, queue string) error
    ResumeQueue(ctx context.Context, queue string) error
    PurgeQueue(ctx context.Context, queue string) error
    GetQueueStats(ctx context.Context) map[string]*QueueStats
    
    // Monitoring
    GetMetrics(ctx context.Context) (*Metrics, error)
    HealthCheck(ctx context.Context) error
    
    // Cleanup
    Cleanup(ctx context.Context) error
    Close() error
}

// Job represents a queue job
type Job struct {
    ID             string                 `json:"id"`
    Type           string                 `json:"type"`
    Payload        interface{}            `json:"payload"`
    Priority       Priority               `json:"priority"`
    Queue          string                 `json:"queue"`
    Delay          time.Duration          `json:"delay"`
    MaxRetries     int                    `json:"max_retries"`
    Timeout        time.Duration          `json:"timeout"`
    BatchKey       string                 `json:"batch_key,omitempty"`
    ParallelConfig *ParallelJobConfig     `json:"parallel_config,omitempty"`
    CreatedAt      time.Time              `json:"created_at"`
    UpdatedAt      time.Time              `json:"updated_at"`
    Status         JobStatus              `json:"status"`
    Error          string                 `json:"error,omitempty"`
    Result         interface{}            `json:"result,omitempty"`
    Attempts       int                    `json:"attempts"`
    LastAttemptAt  *time.Time             `json:"last_attempt_at,omitempty"`
    CompletedAt    *time.Time             `json:"completed_at,omitempty"`
}

// JobHandler handles individual jobs
type JobHandler func(ctx context.Context, job *Job) error

// BatchJobHandler handles batch jobs
type BatchJobHandler func(ctx context.Context, jobs []*Job) error

// ParallelJobHandler handles parallel jobs
type ParallelJobHandler func(ctx context.Context, job *Job) error

// Priority levels
type Priority int

const (
    PriorityLow Priority = iota
    PriorityNormal
    PriorityHigh
    PriorityCritical
)

// Job status
type JobStatus string

const (
    JobStatusPending    JobStatus = "pending"
    JobStatusProcessing JobStatus = "processing"
    JobStatusCompleted  JobStatus = "completed"
    JobStatusFailed     JobStatus = "failed"
    JobStatusCancelled  JobStatus = "cancelled"
    JobStatusRetrying   JobStatus = "retrying"
)

// QueueStats provides queue statistics
type QueueStats struct {
    Name       string `json:"name"`
    Pending    int    `json:"pending"`
    Processing int    `json:"processing"`
    Completed  int    `json:"completed"`
    Failed     int    `json:"failed"`
    Workers    int    `json:"workers"`
    Paused     bool   `json:"paused"`
}

// Metrics provides queue metrics
type Metrics struct {
    TotalJobs      int64         `json:"total_jobs"`
    CompletedJobs  int64         `json:"completed_jobs"`
    FailedJobs     int64         `json:"failed_jobs"`
    ActiveWorkers  int           `json:"active_workers"`
    QueueSizes     map[string]int `json:"queue_sizes"`
    AverageLatency time.Duration `json:"average_latency"`
    Throughput     float64       `json:"throughput"`
}
```

## Testing Utilities

<Tabs items={['Unit Tests', 'Integration Tests', 'Mock Queue', 'Benchmark Tests']}>
  <Tab value="Unit Tests">
    ```go
    package queue_test

    import (
        "context"
        "testing"
        "time"

        "github.com/forge-framework/forge/v2/extensions/queue"
        "github.com/stretchr/testify/assert"
        "github.com/stretchr/testify/require"
    )

    func TestJobEnqueue(t *testing.T) {
        // Create in-memory queue for testing
        queueManager := queue.NewManager(
            queue.WithMemoryBackend(),
        )
        
        ctx := context.Background()
        
        // Test job enqueue
        job := &queue.Job{
            Type: "test_job",
            Payload: map[string]interface{}{
                "message": "hello world",
            },
        }
        
        jobID, err := queueManager.Enqueue(ctx, job)
        require.NoError(t, err)
        assert.NotEmpty(t, jobID)
        
        // Verify job was enqueued
        retrievedJob, err := queueManager.GetJob(ctx, jobID)
        require.NoError(t, err)
        assert.Equal(t, job.Type, retrievedJob.Type)
        assert.Equal(t, job.Payload, retrievedJob.Payload)
    }

    func TestJobProcessing(t *testing.T) {
        queueManager := queue.NewManager(
            queue.WithMemoryBackend(),
        )
        
        ctx := context.Background()
        processed := make(chan bool, 1)
        
        // Register handler
        queueManager.RegisterHandler("test_job", func(ctx context.Context, job *queue.Job) error {
            processed <- true
            return nil
        })
        
        // Start workers
        err := queueManager.StartWorkers(ctx)
        require.NoError(t, err)
        defer queueManager.StopWorkers(ctx)
        
        // Enqueue job
        job := &queue.Job{
            Type: "test_job",
            Payload: map[string]interface{}{
                "test": true,
            },
        }
        
        _, err = queueManager.Enqueue(ctx, job)
        require.NoError(t, err)
        
        // Wait for processing
        select {
        case <-processed:
            // Job processed successfully
        case <-time.After(5 * time.Second):
            t.Fatal("Job was not processed within timeout")
        }
    }

    func TestJobRetry(t *testing.T) {
        queueManager := queue.NewManager(
            queue.WithMemoryBackend(),
        )
        
        ctx := context.Background()
        attempts := 0
        
        // Register handler that fails first time
        queueManager.RegisterHandler("retry_job", func(ctx context.Context, job *queue.Job) error {
            attempts++
            if attempts == 1 {
                return errors.New("first attempt fails")
            }
            return nil
        })
        
        // Start workers
        err := queueManager.StartWorkers(ctx)
        require.NoError(t, err)
        defer queueManager.StopWorkers(ctx)
        
        // Enqueue job with retry
        job := &queue.Job{
            Type:       "retry_job",
            MaxRetries: 2,
            Payload:    map[string]interface{}{},
        }
        
        jobID, err := queueManager.Enqueue(ctx, job)
        require.NoError(t, err)
        
        // Wait for completion
        time.Sleep(2 * time.Second)
        
        // Verify job completed after retry
        retrievedJob, err := queueManager.GetJob(ctx, jobID)
        require.NoError(t, err)
        assert.Equal(t, queue.JobStatusCompleted, retrievedJob.Status)
        assert.Equal(t, 2, attempts)
    }

    func TestPriorityQueue(t *testing.T) {
        queueManager := queue.NewManager(
            queue.WithMemoryBackend(),
            queue.WithPriorityQueues([]string{"high", "normal", "low"}),
        )
        
        ctx := context.Background()
        processOrder := make([]string, 0)
        
        // Register handler that records processing order
        queueManager.RegisterHandler("priority_job", func(ctx context.Context, job *queue.Job) error {
            processOrder = append(processOrder, job.Payload.(map[string]interface{})["id"].(string))
            return nil
        })
        
        // Start workers
        err := queueManager.StartWorkers(ctx)
        require.NoError(t, err)
        defer queueManager.StopWorkers(ctx)
        
        // Enqueue jobs with different priorities
        jobs := []*queue.Job{
            {Type: "priority_job", Priority: queue.PriorityLow, Payload: map[string]interface{}{"id": "low1"}},
            {Type: "priority_job", Priority: queue.PriorityHigh, Payload: map[string]interface{}{"id": "high1"}},
            {Type: "priority_job", Priority: queue.PriorityNormal, Payload: map[string]interface{}{"id": "normal1"}},
            {Type: "priority_job", Priority: queue.PriorityHigh, Payload: map[string]interface{}{"id": "high2"}},
        }
        
        for _, job := range jobs {
            _, err := queueManager.Enqueue(ctx, job)
            require.NoError(t, err)
        }
        
        // Wait for processing
        time.Sleep(2 * time.Second)
        
        // Verify high priority jobs were processed first
        assert.Equal(t, "high1", processOrder[0])
        assert.Equal(t, "high2", processOrder[1])
    }
    ```
  </Tab>
  <Tab value="Integration Tests">
    ```go
    package queue_test

    import (
        "context"
        "testing"
        "time"

        "github.com/forge-framework/forge/v2"
        "github.com/forge-framework/forge/v2/extensions/queue"
        "github.com/stretchr/testify/assert"
        "github.com/stretchr/testify/require"
        "github.com/testcontainers/testcontainers-go"
        "github.com/testcontainers/testcontainers-go/wait"
    )

    func TestQueueIntegrationWithRedis(t *testing.T) {
        // Start Redis container for testing
        ctx := context.Background()
        
        redisContainer, err := testcontainers.GenericContainer(ctx, testcontainers.GenericContainerRequest{
            ContainerRequest: testcontainers.ContainerRequest{
                Image:        "redis:7-alpine",
                ExposedPorts: []string{"6379/tcp"},
                WaitingFor:   wait.ForLog("Ready to accept connections"),
            },
            Started: true,
        })
        require.NoError(t, err)
        defer redisContainer.Terminate(ctx)
        
        // Get Redis connection details
        redisHost, err := redisContainer.Host(ctx)
        require.NoError(t, err)
        redisPort, err := redisContainer.MappedPort(ctx, "6379")
        require.NoError(t, err)
        
        redisURL := fmt.Sprintf("redis://%s:%s", redisHost, redisPort.Port())
        
        // Create Forge app with Queue extension
        app := forge.NewApp()
        app.RegisterExtension(queue.NewExtension(
            queue.WithRedisBackend(redisURL),
        ))
        
        err = app.Start(ctx)
        require.NoError(t, err)
        defer app.Stop(ctx)
        
        queueManager := app.GetExtension("queue").(*queue.Extension).GetManager()
        
        // Test job processing across multiple workers
        processed := make(chan string, 10)
        
        queueManager.RegisterHandler("integration_job", func(ctx context.Context, job *queue.Job) error {
            jobID := job.Payload.(map[string]interface{})["id"].(string)
            processed <- jobID
            return nil
        })
        
        err = queueManager.StartWorkers(ctx)
        require.NoError(t, err)
        
        // Enqueue multiple jobs
        for i := 0; i < 10; i++ {
            job := &queue.Job{
                Type: "integration_job",
                Payload: map[string]interface{}{
                    "id": fmt.Sprintf("job_%d", i),
                },
            }
            
            _, err := queueManager.Enqueue(ctx, job)
            require.NoError(t, err)
        }
        
        // Collect processed jobs
        processedJobs := make([]string, 0)
        timeout := time.After(10 * time.Second)
        
        for len(processedJobs) < 10 {
            select {
            case jobID := <-processed:
                processedJobs = append(processedJobs, jobID)
            case <-timeout:
                t.Fatalf("Not all jobs processed within timeout. Processed: %d/10", len(processedJobs))
            }
        }
        
        assert.Len(t, processedJobs, 10)
    }

    func TestScheduledJobsIntegration(t *testing.T) {
        ctx := context.Background()
        
        app := forge.NewApp()
        app.RegisterExtension(queue.NewExtension(
            queue.WithMemoryBackend(),
            queue.WithScheduler(queue.SchedulerConfig{
                Enabled:  true,
                Timezone: "UTC",
            }),
        ))
        
        err := app.Start(ctx)
        require.NoError(t, err)
        defer app.Stop(ctx)
        
        queueManager := app.GetExtension("queue").(*queue.Extension).GetManager()
        
        executed := make(chan bool, 1)
        
        queueManager.RegisterHandler("scheduled_job", func(ctx context.Context, job *queue.Job) error {
            executed <- true
            return nil
        })
        
        err = queueManager.StartWorkers(ctx)
        require.NoError(t, err)
        
        // Schedule job to run in 2 seconds
        job := &queue.Job{
            Type: "scheduled_job",
            Payload: map[string]interface{}{
                "message": "scheduled execution",
            },
        }
        
        _, err = queueManager.ScheduleIn(ctx, 2*time.Second, job)
        require.NoError(t, err)
        
        // Wait for execution
        select {
        case <-executed:
            // Job executed successfully
        case <-time.After(5 * time.Second):
            t.Fatal("Scheduled job was not executed within timeout")
        }
    }

    func TestBatchProcessingIntegration(t *testing.T) {
        ctx := context.Background()
        
        app := forge.NewApp()
        app.RegisterExtension(queue.NewExtension(
            queue.WithMemoryBackend(),
            queue.WithBatchProcessing(queue.BatchConfig{
                Enabled:     true,
                BatchSize:   5,
                MaxWaitTime: 2 * time.Second,
            }),
        ))
        
        err := app.Start(ctx)
        require.NoError(t, err)
        defer app.Stop(ctx)
        
        queueManager := app.GetExtension("queue").(*queue.Extension).GetManager()
        
        batchProcessed := make(chan int, 1)
        
        queueManager.RegisterBatchHandler("batch_job", func(ctx context.Context, jobs []*queue.Job) error {
            batchProcessed <- len(jobs)
            return nil
        })
        
        err = queueManager.StartWorkers(ctx)
        require.NoError(t, err)
        
        // Enqueue jobs for batching
        for i := 0; i < 5; i++ {
            job := &queue.Job{
                Type:     "batch_job",
                BatchKey: "test_batch",
                Payload: map[string]interface{}{
                    "item": i,
                },
            }
            
            _, err := queueManager.Enqueue(ctx, job)
            require.NoError(t, err)
        }
        
        // Wait for batch processing
        select {
        case batchSize := <-batchProcessed:
            assert.Equal(t, 5, batchSize)
        case <-time.After(5 * time.Second):
            t.Fatal("Batch was not processed within timeout")
        }
    }
    ```
  </Tab>
  <Tab value="Mock Queue">
    ```go
    package queue_test

    import (
        "context"
        "sync"
        "time"

        "github.com/forge-framework/forge/v2/extensions/queue"
    )

    // MockQueueManager provides a mock implementation for testing
    type MockQueueManager struct {
        jobs     map[string]*queue.Job
        handlers map[string]queue.JobHandler
        mu       sync.RWMutex
        
        // Test helpers
        EnqueuedJobs   []*queue.Job
        ProcessedJobs  []*queue.Job
        FailedJobs     []*queue.Job
        ShouldFail     bool
        ProcessDelay   time.Duration
    }

    func NewMockQueueManager() *MockQueueManager {
        return &MockQueueManager{
            jobs:     make(map[string]*queue.Job),
            handlers: make(map[string]queue.JobHandler),
        }
    }

    func (m *MockQueueManager) Enqueue(ctx context.Context, job *queue.Job) (string, error) {
        m.mu.Lock()
        defer m.mu.Unlock()
        
        job.ID = generateJobID()
        job.Status = queue.JobStatusPending
        job.CreatedAt = time.Now()
        
        m.jobs[job.ID] = job
        m.EnqueuedJobs = append(m.EnqueuedJobs, job)
        
        // Auto-process if handler exists
        if handler, exists := m.handlers[job.Type]; exists {
            go m.processJob(ctx, job, handler)
        }
        
        return job.ID, nil
    }

    func (m *MockQueueManager) RegisterHandler(jobType string, handler queue.JobHandler) error {
        m.mu.Lock()
        defer m.mu.Unlock()
        
        m.handlers[jobType] = handler
        return nil
    }

    func (m *MockQueueManager) GetJob(ctx context.Context, jobID string) (*queue.Job, error) {
        m.mu.RLock()
        defer m.mu.RUnlock()
        
        job, exists := m.jobs[jobID]
        if !exists {
            return nil, errors.New("job not found")
        }
        
        return job, nil
    }

    func (m *MockQueueManager) processJob(ctx context.Context, job *queue.Job, handler queue.JobHandler) {
        if m.ProcessDelay > 0 {
            time.Sleep(m.ProcessDelay)
        }
        
        m.mu.Lock()
        job.Status = queue.JobStatusProcessing
        m.mu.Unlock()
        
        var err error
        if m.ShouldFail {
            err = errors.New("mock failure")
        } else {
            err = handler(ctx, job)
        }
        
        m.mu.Lock()
        defer m.mu.Unlock()
        
        if err != nil {
            job.Status = queue.JobStatusFailed
            job.Error = err.Error()
            m.FailedJobs = append(m.FailedJobs, job)
        } else {
            job.Status = queue.JobStatusCompleted
            job.CompletedAt = &time.Time{}
            *job.CompletedAt = time.Now()
            m.ProcessedJobs = append(m.ProcessedJobs, job)
        }
    }

    // Test helper methods
    func (m *MockQueueManager) GetEnqueuedJobCount() int {
        m.mu.RLock()
        defer m.mu.RUnlock()
        return len(m.EnqueuedJobs)
    }

    func (m *MockQueueManager) GetProcessedJobCount() int {
        m.mu.RLock()
        defer m.mu.RUnlock()
        return len(m.ProcessedJobs)
    }

    func (m *MockQueueManager) GetFailedJobCount() int {
        m.mu.RLock()
        defer m.mu.RUnlock()
        return len(m.FailedJobs)
    }

    func (m *MockQueueManager) Reset() {
        m.mu.Lock()
        defer m.mu.Unlock()
        
        m.jobs = make(map[string]*queue.Job)
        m.EnqueuedJobs = nil
        m.ProcessedJobs = nil
        m.FailedJobs = nil
        m.ShouldFail = false
        m.ProcessDelay = 0
    }

    // Example usage in tests
    func TestWithMockQueue(t *testing.T) {
        mockQueue := NewMockQueueManager()
        
        // Register handler
        mockQueue.RegisterHandler("test_job", func(ctx context.Context, job *queue.Job) error {
            // Test handler logic
            return nil
        })
        
        // Enqueue job
        job := &queue.Job{
            Type: "test_job",
            Payload: map[string]interface{}{
                "test": true,
            },
        }
        
        jobID, err := mockQueue.Enqueue(context.Background(), job)
        require.NoError(t, err)
        
        // Wait for processing
        time.Sleep(100 * time.Millisecond)
        
        // Verify results
        assert.Equal(t, 1, mockQueue.GetEnqueuedJobCount())
        assert.Equal(t, 1, mockQueue.GetProcessedJobCount())
        assert.Equal(t, 0, mockQueue.GetFailedJobCount())
        
        // Verify job status
        processedJob, err := mockQueue.GetJob(context.Background(), jobID)
        require.NoError(t, err)
        assert.Equal(t, queue.JobStatusCompleted, processedJob.Status)
    }

    func generateJobID() string {
        return fmt.Sprintf("job_%d", time.Now().UnixNano())
    }
    ```
  </Tab>
  <Tab value="Benchmark Tests">
    ```go
    package queue_test

    import (
        "context"
        "testing"
        "time"

        "github.com/forge-framework/forge/v2/extensions/queue"
    )

    func BenchmarkJobEnqueue(b *testing.B) {
        queueManager := queue.NewManager(
            queue.WithMemoryBackend(),
        )
        
        ctx := context.Background()
        
        job := &queue.Job{
            Type: "benchmark_job",
            Payload: map[string]interface{}{
                "data": "test data",
            },
        }
        
        b.ResetTimer()
        b.RunParallel(func(pb *testing.PB) {
            for pb.Next() {
                _, err := queueManager.Enqueue(ctx, job)
                if err != nil {
                    b.Fatal(err)
                }
            }
        })
    }

    func BenchmarkJobProcessing(b *testing.B) {
        queueManager := queue.NewManager(
            queue.WithMemoryBackend(),
            queue.WithWorkerPool(queue.WorkerPoolConfig{
                Workers:     10,
                Concurrency: 20,
            }),
        )
        
        ctx := context.Background()
        processed := make(chan bool, b.N)
        
        // Register fast handler
        queueManager.RegisterHandler("benchmark_job", func(ctx context.Context, job *queue.Job) error {
            processed <- true
            return nil
        })
        
        err := queueManager.StartWorkers(ctx)
        if err != nil {
            b.Fatal(err)
        }
        defer queueManager.StopWorkers(ctx)
        
        b.ResetTimer()
        
        // Enqueue jobs
        for i := 0; i < b.N; i++ {
            job := &queue.Job{
                Type: "benchmark_job",
                Payload: map[string]interface{}{
                    "id": i,
                },
            }
            
            _, err := queueManager.Enqueue(ctx, job)
            if err != nil {
                b.Fatal(err)
            }
        }
        
        // Wait for all jobs to be processed
        for i := 0; i < b.N; i++ {
            select {
            case <-processed:
                // Job processed
            case <-time.After(30 * time.Second):
                b.Fatalf("Timeout waiting for job %d", i)
            }
        }
    }

    func BenchmarkBatchProcessing(b *testing.B) {
        queueManager := queue.NewManager(
            queue.WithMemoryBackend(),
            queue.WithBatchProcessing(queue.BatchConfig{
                Enabled:     true,
                BatchSize:   100,
                MaxWaitTime: 10 * time.Millisecond,
            }),
        )
        
        ctx := context.Background()
        processed := make(chan int, b.N/100+1)
        
        // Register batch handler
        queueManager.RegisterBatchHandler("benchmark_batch", func(ctx context.Context, jobs []*queue.Job) error {
            processed <- len(jobs)
            return nil
        })
        
        err := queueManager.StartWorkers(ctx)
        if err != nil {
            b.Fatal(err)
        }
        defer queueManager.StopWorkers(ctx)
        
        b.ResetTimer()
        
        // Enqueue jobs for batching
        for i := 0; i < b.N; i++ {
            job := &queue.Job{
                Type:     "benchmark_batch",
                BatchKey: "bench_batch",
                Payload: map[string]interface{}{
                    "id": i,
                },
            }
            
            _, err := queueManager.Enqueue(ctx, job)
            if err != nil {
                b.Fatal(err)
            }
        }
        
        // Wait for batch processing
        totalProcessed := 0
        timeout := time.After(30 * time.Second)
        
        for totalProcessed < b.N {
            select {
            case batchSize := <-processed:
                totalProcessed += batchSize
            case <-timeout:
                b.Fatalf("Timeout waiting for batch processing. Processed: %d/%d", totalProcessed, b.N)
            }
        }
    }

    func BenchmarkPriorityQueue(b *testing.B) {
        queueManager := queue.NewManager(
            queue.WithMemoryBackend(),
            queue.WithPriorityQueues([]string{"high", "normal", "low"}),
        )
        
        ctx := context.Background()
        
        b.ResetTimer()
        b.RunParallel(func(pb *testing.PB) {
            for pb.Next() {
                job := &queue.Job{
                    Type:     "priority_job",
                    Priority: queue.PriorityHigh,
                    Payload: map[string]interface{}{
                        "data": "test",
                    },
                }
                
                _, err := queueManager.EnqueueToPriority(ctx, "high", job)
                if err != nil {
                    b.Fatal(err)
                }
            }
        })
    }
    ```
  </Tab>
</Tabs>

## Best Practices

<Callout type="info">
  Follow these best practices to ensure optimal queue performance and reliability.
</Callout>

### Job Design

```go
// Good: Small, focused job payload
type EmailJob struct {
    UserID    int64  `json:"user_id"`
    Template  string `json:"template"`
    Variables map[string]string `json:"variables"`
}

// Bad: Large, complex payload
type BadJob struct {
    UserData    User              `json:"user_data"`     // Large object
    EmailHTML   string            `json:"email_html"`    // Large content
    Attachments [][]byte          `json:"attachments"`   // Binary data
    History     []EmailHistory    `json:"history"`       // Unnecessary data
}

// Good: Idempotent job design
func ProcessPayment(ctx context.Context, job *queue.Job) error {
    paymentID := job.Payload["payment_id"].(string)
    
    // Check if already processed
    if isPaymentProcessed(paymentID) {
        return nil // Already processed, skip
    }
    
    // Process payment
    return processPayment(paymentID)
}

// Good: Proper error handling
func HandleEmailJob(ctx context.Context, job *queue.Job) error {
    email := parseEmailJob(job)
    
    if err := validateEmail(email); err != nil {
        // Don't retry validation errors
        return queue.NewPermanentError(err)
    }
    
    if err := sendEmail(email); err != nil {
        // Retry transient errors
        if isTransientError(err) {
            return err
        }
        // Don't retry permanent errors
        return queue.NewPermanentError(err)
    }
    
    return nil
}
```

### Error Handling

```go
// Define custom error types
type PermanentError struct {
    Err error
}

func (e PermanentError) Error() string {
    return e.Err.Error()
}

func (e PermanentError) IsPermanent() bool {
    return true
}

// Use exponential backoff for retries
func configureRetries() queue.Extension {
    return queue.NewExtension(
        queue.WithRetryPolicy(queue.RetryPolicy{
            MaxRetries:      5,
            InitialDelay:    time.Second,
            MaxDelay:        5 * time.Minute,
            BackoffFactor:   2.0,
            Jitter:          true,
        }),
    )
}

// Implement circuit breaker pattern
type CircuitBreakerHandler struct {
    handler queue.JobHandler
    breaker *CircuitBreaker
}

func (h *CircuitBreakerHandler) Handle(ctx context.Context, job *queue.Job) error {
    return h.breaker.Execute(func() error {
        return h.handler(ctx, job)
    })
}
```

### Performance Optimization

```go
// Use connection pooling
func optimizeRedisBackend() queue.Extension {
    return queue.NewExtension(
        queue.WithRedisBackend("redis://localhost:6379"),
        queue.WithRedisPool(20),              // Pool size
        queue.WithRedisIdleTimeout(5*time.Minute),
        queue.WithRedisMaxConnAge(30*time.Minute),
    )
}

// Optimize worker configuration
func optimizeWorkers() queue.Extension {
    return queue.NewExtension(
        queue.WithWorkerPool(queue.WorkerPoolConfig{
            Workers:             runtime.NumCPU() * 2,
            Concurrency:         50,
            PrefetchCount:       10,
            ShutdownTimeout:     30 * time.Second,
            HealthCheckInterval: 10 * time.Second,
        }),
    )
}

// Use batch processing for similar jobs
func setupBatchOptimization() queue.Extension {
    return queue.NewExtension(
        queue.WithBatchProcessing(queue.BatchConfig{
            Enabled:      true,
            BatchSize:    100,
            MaxWaitTime:  5 * time.Second,
            MaxBatchSize: 1000,
        }),
    )
}

// Monitor and auto-scale
func setupAutoScaling(queueManager queue.QueueManager) {
    go func() {
        ticker := time.NewTicker(30 * time.Second)
        defer ticker.Stop()
        
        for {
            select {
            case <-ticker.C:
                stats := queueManager.GetQueueStats(context.Background())
                
                for queueName, stat := range stats {
                    // Scale up if queue is backing up
                    if stat.Pending > 100 && stat.Workers < 20 {
                        queueManager.ScaleWorkers(context.Background(), queueName, stat.Workers+2)
                    }
                    
                    // Scale down if queue is empty
                    if stat.Pending < 10 && stat.Workers > 2 {
                        queueManager.ScaleWorkers(context.Background(), queueName, stat.Workers-1)
                    }
                }
            }
        }
    }()
}
```

### Security

```go
// Encrypt sensitive job data
func setupEncryption() queue.Extension {
    return queue.NewExtension(
        queue.WithSecurity(queue.SecurityConfig{
            EncryptionEnabled: true,
            EncryptionKey:     os.Getenv("QUEUE_ENCRYPTION_KEY"),
            AuthRequired:      true,
            AllowedJobTypes:   []string{"email", "notification", "report"},
        }),
    )
}

// Validate job types
func validateJobHandler(jobType string, handler queue.JobHandler) queue.JobHandler {
    allowedTypes := map[string]bool{
        "send_email":      true,
        "process_payment": true,
        "generate_report": true,
    }
    
    return func(ctx context.Context, job *queue.Job) error {
        if !allowedTypes[jobType] {
            return queue.NewPermanentError(fmt.Errorf("unauthorized job type: %s", jobType))
        }
        
        return handler(ctx, job)
    }
}

// Sanitize job payloads
func sanitizePayload(payload interface{}) interface{} {
    // Remove sensitive fields
    if data, ok := payload.(map[string]interface{}); ok {
        delete(data, "password")
        delete(data, "secret")
        delete(data, "token")
        delete(data, "api_key")
    }
    
    return payload
}
```

<Callout type="warning">
  Always validate and sanitize job payloads to prevent security vulnerabilities and ensure data integrity.
</Callout>

## Troubleshooting

### Common Issues

<Tabs items={['Connection Issues', 'Performance Problems', 'Job Failures', 'Memory Issues']}>
  <Tab value="Connection Issues">
    ```go
    // Redis connection issues
    func debugRedisConnection() {
        // Check Redis connectivity
        client := redis.NewClient(&redis.Options{
            Addr: "localhost:6379",
        })
        
        ctx := context.Background()
        pong, err := client.Ping(ctx).Result()
        if err != nil {
            log.Printf("Redis connection failed: %v", err)
            return
        }
        log.Printf("Redis connected: %s", pong)
        
        // Check Redis memory usage
        info, err := client.Info(ctx, "memory").Result()
        if err != nil {
            log.Printf("Failed to get Redis info: %v", err)
            return
        }
        log.Printf("Redis memory info: %s", info)
    }

    // Connection pool monitoring
    func monitorConnectionPool(queueManager queue.QueueManager) {
        go func() {
            ticker := time.NewTicker(30 * time.Second)
            defer ticker.Stop()
            
            for {
                select {
                case <-ticker.C:
                    stats := queueManager.GetConnectionStats()
                    log.Printf("Connection pool stats: active=%d, idle=%d, total=%d",
                        stats.ActiveConnections,
                        stats.IdleConnections,
                        stats.TotalConnections,
                    )
                    
                    if stats.ActiveConnections > stats.MaxConnections*0.8 {
                        log.Warn("Connection pool near capacity")
                    }
                }
            }
        }()
    }

    // Retry with exponential backoff
    func retryConnection(fn func() error, maxRetries int) error {
        var err error
        for i := 0; i < maxRetries; i++ {
            if err = fn(); err == nil {
                return nil
            }
            
            delay := time.Duration(math.Pow(2, float64(i))) * time.Second
            log.Printf("Connection attempt %d failed: %v. Retrying in %v", i+1, err, delay)
            time.Sleep(delay)
        }
        return fmt.Errorf("failed after %d attempts: %w", maxRetries, err)
    }
    ```
  </Tab>
  <Tab value="Performance Problems">
    ```go
    // Monitor queue performance
    func monitorQueuePerformance(queueManager queue.QueueManager) {
        go func() {
            ticker := time.NewTicker(10 * time.Second)
            defer ticker.Stop()
            
            for {
                select {
                case <-ticker.C:
                    metrics, err := queueManager.GetMetrics(context.Background())
                    if err != nil {
                        log.Printf("Failed to get metrics: %v", err)
                        continue
                    }
                    
                    // Check for performance issues
                    if metrics.AverageLatency > 5*time.Second {
                        log.Warn("High average latency detected: %v", metrics.AverageLatency)
                    }
                    
                    if metrics.Throughput < 10 {
                        log.Warn("Low throughput detected: %.2f jobs/sec", metrics.Throughput)
                    }
                    
                    // Check queue sizes
                    for queueName, size := range metrics.QueueSizes {
                        if size > 1000 {
                            log.Warn("Queue %s has %d pending jobs", queueName, size)
                        }
                    }
                }
            }
        }()
    }

    // Optimize worker allocation
    func optimizeWorkerAllocation(queueManager queue.QueueManager) {
        stats := queueManager.GetQueueStats(context.Background())
        
        for queueName, stat := range stats {
            // Calculate optimal worker count
            optimalWorkers := calculateOptimalWorkers(stat)
            
            if optimalWorkers != stat.Workers {
                log.Printf("Scaling queue %s from %d to %d workers",
                    queueName, stat.Workers, optimalWorkers)
                
                err := queueManager.ScaleWorkers(context.Background(), queueName, optimalWorkers)
                if err != nil {
                    log.Printf("Failed to scale workers: %v", err)
                }
            }
        }
    }

    func calculateOptimalWorkers(stat *queue.QueueStats) int {
        // Simple heuristic: 1 worker per 50 pending jobs, min 1, max 20
        workers := stat.Pending / 50
        if workers < 1 {
            workers = 1
        }
        if workers > 20 {
            workers = 20
        }
        return workers
    }

    // Memory usage optimization
    func optimizeMemoryUsage() queue.Extension {
        return queue.NewExtension(
            // Use smaller batch sizes to reduce memory
            queue.WithBatchProcessing(queue.BatchConfig{
                BatchSize:    25,  // Reduced from 100
                MaxBatchSize: 100, // Reduced from 1000
            }),
            
            // Limit job payload size
            queue.WithJobSizeLimit(1024 * 1024), // 1MB limit
            
            // Enable compression for large payloads
            queue.WithCompression(true),
            
            // Shorter retention period
            queue.WithPersistence(queue.PersistenceConfig{
                RetentionPeriod: 24 * time.Hour, // Reduced from 7 days
            }),
        )
    }
    ```
  </Tab>
  <Tab value="Job Failures">
    ```go
    // Debug job failures
    func debugJobFailures(queueManager queue.QueueManager) {
        // Get failed jobs
        failedJobs, err := queueManager.GetFailedJobs(context.Background(), 100)
        if err != nil {
            log.Printf("Failed to get failed jobs: %v", err)
            return
        }
        
        // Analyze failure patterns
        errorCounts := make(map[string]int)
        jobTypeCounts := make(map[string]int)
        
        for _, job := range failedJobs {
            errorCounts[job.Error]++
            jobTypeCounts[job.Type]++
        }
        
        log.Printf("Error analysis:")
        for error, count := range errorCounts {
            log.Printf("  %s: %d occurrences", error, count)
        }
        
        log.Printf("Job type analysis:")
        for jobType, count := range jobTypeCounts {
            log.Printf("  %s: %d failures", jobType, count)
        }
    }

    // Implement dead letter queue processing
    func processDLQ(queueManager queue.QueueManager) {
        queueManager.RegisterHandler("dlq_processor", func(ctx context.Context, job *queue.Job) error {
            log.Printf("Processing DLQ job: %s", job.ID)
            
            // Analyze the failure
            if shouldRetry(job) {
                // Reset retry count and re-enqueue
                job.Attempts = 0
                job.Status = queue.JobStatusPending
                
                _, err := queueManager.Enqueue(ctx, job)
                if err != nil {
                    log.Printf("Failed to re-enqueue job: %v", err)
                    return err
                }
                
                log.Printf("Re-enqueued job %s", job.ID)
            } else {
                // Log for manual investigation
                log.Printf("Job %s requires manual investigation: %s", job.ID, job.Error)
                
                // Store in permanent failure log
                storeFailedJob(job)
            }
            
            return nil
        })
    }

    func shouldRetry(job *queue.Job) bool {
        // Don't retry validation errors
        if strings.Contains(job.Error, "validation") {
            return false
        }
        
        // Don't retry authentication errors
        if strings.Contains(job.Error, "unauthorized") {
            return false
        }
        
        // Retry network errors
        if strings.Contains(job.Error, "connection") {
            return true
        }
        
        // Retry timeout errors
        if strings.Contains(job.Error, "timeout") {
            return true
        }
        
        return false
    }

    // Job timeout handling
    func handleJobTimeouts(queueManager queue.QueueManager) {
        queueManager.RegisterHandler("timeout_handler", func(ctx context.Context, job *queue.Job) error {
            // Create context with timeout
            jobCtx, cancel := context.WithTimeout(ctx, job.Timeout)
            defer cancel()
            
            // Execute job with timeout
            done := make(chan error, 1)
            go func() {
                done <- executeJob(jobCtx, job)
            }()
            
            select {
            case err := <-done:
                return err
            case <-jobCtx.Done():
                return fmt.Errorf("job timed out after %v", job.Timeout)
            }
        })
    }
    ```
  </Tab>
  <Tab value="Memory Issues">
    ```go
    // Monitor memory usage
    func monitorMemoryUsage() {
        go func() {
            ticker := time.NewTicker(30 * time.Second)
            defer ticker.Stop()
            
            for {
                select {
                case <-ticker.C:
                    var m runtime.MemStats
                    runtime.ReadMemStats(&m)
                    
                    log.Printf("Memory stats: Alloc=%d KB, TotalAlloc=%d KB, Sys=%d KB, NumGC=%d",
                        bToKb(m.Alloc),
                        bToKb(m.TotalAlloc),
                        bToKb(m.Sys),
                        m.NumGC,
                    )
                    
                    // Force GC if memory usage is high
                    if m.Alloc > 500*1024*1024 { // 500MB
                        log.Printf("High memory usage detected, forcing GC")
                        runtime.GC()
                    }
                }
            }
        }()
    }

    func bToKb(b uint64) uint64 {
        return b / 1024
    }

    // Implement job payload streaming for large jobs
    type StreamingJob struct {
        ID       string
        Type     string
        DataURL  string // URL to fetch actual data
        Metadata map[string]interface{}
    }

    func handleStreamingJob(ctx context.Context, job *queue.Job) error {
        streamingJob := job.Payload.(*StreamingJob)
        
        // Stream data instead of loading into memory
        resp, err := http.Get(streamingJob.DataURL)
        if err != nil {
            return err
        }
        defer resp.Body.Close()
        
        // Process data in chunks
        buffer := make([]byte, 8192)
        for {
            n, err := resp.Body.Read(buffer)
            if err == io.EOF {
                break
            }
            if err != nil {
                return err
            }
            
            // Process chunk
            if err := processChunk(buffer[:n]); err != nil {
                return err
            }
        }
        
        return nil
    }

    // Memory-efficient batch processing
    func processLargeBatch(ctx context.Context, jobs []*queue.Job) error {
        // Process in smaller sub-batches to control memory
        const subBatchSize = 10
        
        for i := 0; i < len(jobs); i += subBatchSize {
            end := i + subBatchSize
            if end > len(jobs) {
                end = len(jobs)
            }
            
            subBatch := jobs[i:end]
            if err := processSubBatch(ctx, subBatch); err != nil {
                return err
            }
            
            // Force GC between sub-batches
            runtime.GC()
        }
        
        return nil
    }
    ```
  </Tab>
</Tabs>

### Debugging Tools

```go
// Queue inspector for debugging
type QueueInspector struct {
    manager queue.QueueManager
}

func NewQueueInspector(manager queue.QueueManager) *QueueInspector {
    return &QueueInspector{manager: manager}
}

func (qi *QueueInspector) InspectQueue(queueName string) {
    ctx := context.Background()
    
    // Get queue statistics
    stats := qi.manager.GetQueueStats(ctx)
    if queueStats, exists := stats[queueName]; exists {
        log.Printf("Queue %s stats:", queueName)
        log.Printf("  Pending: %d", queueStats.Pending)
        log.Printf("  Processing: %d", queueStats.Processing)
        log.Printf("  Completed: %d", queueStats.Completed)
        log.Printf("  Failed: %d", queueStats.Failed)
        log.Printf("  Workers: %d", queueStats.Workers)
        log.Printf("  Paused: %t", queueStats.Paused)
    }
    
    // Get recent jobs
    recentJobs, err := qi.manager.GetRecentJobs(ctx, queueName, 10)
    if err != nil {
        log.Printf("Failed to get recent jobs: %v", err)
        return
    }
    
    log.Printf("Recent jobs in %s:", queueName)
    for _, job := range recentJobs {
        log.Printf("  Job %s: %s (%s)", job.ID, job.Type, job.Status)
        if job.Error != "" {
            log.Printf("    Error: %s", job.Error)
        }
    }
}

func (qi *QueueInspector) InspectJob(jobID string) {
    ctx := context.Background()
    
    job, err := qi.manager.GetJob(ctx, jobID)
    if err != nil {
        log.Printf("Failed to get job %s: %v", jobID, err)
        return
    }
    
    log.Printf("Job %s details:", jobID)
    log.Printf("  Type: %s", job.Type)
    log.Printf("  Status: %s", job.Status)
    log.Printf("  Queue: %s", job.Queue)
    log.Printf("  Priority: %v", job.Priority)
    log.Printf("  Created: %v", job.CreatedAt)
    log.Printf("  Updated: %v", job.UpdatedAt)
    log.Printf("  Attempts: %d", job.Attempts)
    log.Printf("  Max Retries: %d", job.MaxRetries)
    
    if job.Error != "" {
        log.Printf("  Error: %s", job.Error)
    }
    
    if job.LastAttemptAt != nil {
        log.Printf("  Last Attempt: %v", *job.LastAttemptAt)
    }
    
    if job.CompletedAt != nil {
        log.Printf("  Completed: %v", *job.CompletedAt)
    }
    
    log.Printf("  Payload: %+v", job.Payload)
}

// Health check implementation
func (qi *QueueInspector) HealthCheck() error {
    ctx := context.Background()
    
    // Check queue manager health
    if err := qi.manager.HealthCheck(ctx); err != nil {
        return fmt.Errorf("queue manager health check failed: %w", err)
    }
    
    // Check individual queue health
    stats := qi.manager.GetQueueStats(ctx)
    for queueName, queueStats := range stats {
        // Check if queue is stuck
        if queueStats.Processing > 0 && queueStats.Workers == 0 {
            return fmt.Errorf("queue %s has processing jobs but no workers", queueName)
        }
        
        // Check if queue is backing up
        if queueStats.Pending > 10000 {
            log.Printf("Warning: queue %s has %d pending jobs", queueName, queueStats.Pending)
        }
    }
    
    return nil
}
```

## Migration Guide

### From v1 to v2

<Callout type="info">
  This guide helps you migrate from Queue v1 to v2 with minimal disruption.
</Callout>

```go
// v1 Configuration (deprecated)
func oldQueueConfig() {
    queue := queue.New(queue.Config{
        Backend: "redis",
        URL:     "redis://localhost:6379",
        Workers: 5,
    })
    
    queue.RegisterJob("send_email", sendEmailHandler)
    queue.Start()
}

// v2 Configuration (recommended)
func newQueueConfig() queue.Extension {
    return queue.NewExtension(
        queue.WithRedisBackend("redis://localhost:6379"),
        queue.WithWorkerPool(queue.WorkerPoolConfig{
            Workers: 5,
        }),
    )
}

// Handler migration
// v1 Handler
func oldSendEmailHandler(job queue.Job) error {
    email := job.Data["email"].(string)
    return sendEmail(email)
}

// v2 Handler
func newSendEmailHandler(ctx context.Context, job *queue.Job) error {
    email := job.Payload.(map[string]interface{})["email"].(string)
    return sendEmail(email)
}

// Job enqueuing migration
// v1 Enqueue
func oldEnqueue() {
    job := queue.Job{
        Type: "send_email",
        Data: map[string]interface{}{
            "email": "user@example.com",
        },
    }
    queue.Enqueue(job)
}

// v2 Enqueue
func newEnqueue(queueManager queue.QueueManager) {
    job := &queue.Job{
        Type: "send_email",
        Payload: map[string]interface{}{
            "email": "user@example.com",
        },
    }
    queueManager.Enqueue(context.Background(), job)
}
```

### Breaking Changes

1. **Context Support**: All handlers now require `context.Context` as the first parameter
2. **Job Structure**: Job payload is now `interface{}` instead of `map[string]interface{}`
3. **Error Handling**: Enhanced error types with permanent vs. temporary error distinction
4. **Configuration**: Moved from struct-based to functional options pattern
5. **Extension System**: Queue is now a Forge extension instead of standalone package

### Migration Steps

1. **Update Dependencies**
   ```bash
   go mod edit -require github.com/forge-framework/forge/v2@latest
   go mod tidy
   ```

2. **Update Configuration**
   ```go
   // Replace old queue initialization
   app := forge.NewApp()
   app.RegisterExtension(queue.NewExtension(
       queue.WithRedisBackend("redis://localhost:6379"),
       // ... other options
   ))
   ```

3. **Update Handlers**
   ```go
   // Add context parameter to all handlers
   func myHandler(ctx context.Context, job *queue.Job) error {
       // Handler implementation
       return nil
   }
   ```

4. **Update Job Enqueuing**
   ```go
   // Use QueueManager from extension
   queueExt := app.GetExtension("queue").(*queue.Extension)
   queueManager := queueExt.GetManager()
   
   // Enqueue with context
   _, err := queueManager.Enqueue(ctx, job)
   ```

### Compatibility Layer

```go
// Compatibility wrapper for gradual migration
type CompatibilityWrapper struct {
    manager queue.QueueManager
}

func NewCompatibilityWrapper(manager queue.QueueManager) *CompatibilityWrapper {
    return &CompatibilityWrapper{manager: manager}
}

// v1-style enqueue method
func (cw *CompatibilityWrapper) Enqueue(job OldJob) error {
    newJob := &queue.Job{
        Type:    job.Type,
        Payload: job.Data,
    }
    
    _, err := cw.manager.Enqueue(context.Background(), newJob)
    return err
}

// v1-style handler registration
func (cw *CompatibilityWrapper) RegisterJob(jobType string, handler func(OldJob) error) {
    newHandler := func(ctx context.Context, job *queue.Job) error {
        oldJob := OldJob{
            Type: job.Type,
            Data: job.Payload.(map[string]interface{}),
        }
        return handler(oldJob)
    }
    
    cw.manager.RegisterHandler(jobType, newHandler)
}

type OldJob struct {
    Type string
    Data map[string]interface{}
}
```

## Conclusion

The Queue extension provides a robust, scalable solution for background job processing in Forge applications. With support for multiple backends, advanced features like priority queues and batch processing, and comprehensive monitoring capabilities, it can handle everything from simple async tasks to complex distributed workflows.

### Key Benefits

- **Scalability**: Horizontal scaling with multiple workers and queues
- **Reliability**: Built-in retry mechanisms, dead letter queues, and persistence
- **Flexibility**: Multiple backend options and configuration approaches
- **Observability**: Comprehensive metrics, logging, and health checks
- **Security**: Encryption, authentication, and job type validation
- **Performance**: Optimized for high throughput with minimal latency

### Next Steps

1. **Start Simple**: Begin with in-memory backend for development
2. **Add Persistence**: Move to Redis or database backend for production
3. **Monitor Performance**: Implement metrics and alerting
4. **Scale Gradually**: Add workers and queues as needed
5. **Optimize**: Use batch processing and priority queues for efficiency

<Callout type="tip">
  For production deployments, always enable monitoring, configure appropriate retry policies, and implement proper error handling to ensure reliable job processing.
</Callout>