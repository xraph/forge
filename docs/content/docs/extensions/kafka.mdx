---
title: Kafka Extension
description: High-performance Apache Kafka integration for event streaming and message processing
---

import { Card, Cards } from 'fumadocs-ui/components/card'
import { Callout } from 'fumadocs-ui/components/callout'
import { Tab, Tabs } from 'fumadocs-ui/components/tabs'

The Kafka Extension provides a complete Apache Kafka integration with producers, consumers, streaming capabilities, and advanced features for building event-driven architectures and real-time data processing systems.

## Key Features

<Cards>
  <Card
    title="🚀 High Throughput"
    description="Optimized for high-throughput message processing"
  />
  <Card
    title="📤 Producers"
    description="Synchronous and asynchronous message producers"
  />
  <Card
    title="📥 Consumers"
    description="Consumer groups with automatic partition assignment"
  />
  <Card
    title="🌊 Streaming"
    description="Real-time stream processing with Kafka Streams"
  />
  <Card
    title="🔄 Transactions"
    description="Exactly-once semantics with transactional producers"
  />
  <Card
    title="📊 Monitoring"
    description="Built-in metrics and health checks"
  />
</Cards>

## Installation

```bash
go get github.com/xraph/forge/extensions/kafka
go get github.com/segmentio/kafka-go
```

## Basic Usage

<Tabs items={['Producer', 'Consumer', 'Consumer Group', 'Streaming', 'Transactions']}>
  <Tab value="Producer">
    ```go
    package main
    
    import (
        "context"
        "encoding/json"
        "time"
        
        "github.com/xraph/forge/v2"
        "github.com/xraph/forge/v2/extensions/kafka"
    )
    
    func main() {
        app := forge.NewApp(forge.AppConfig{
            Name: "kafka-producer",
        })
        
        // Register Kafka extension
        app.RegisterExtension(kafka.NewExtension(
            kafka.WithBrokers([]string{"localhost:9092"}),
            kafka.WithProducer(kafka.ProducerConfig{
                Topic:         "user-events",
                Async:         true,
                BatchSize:     100,
                BatchTimeout:  10 * time.Millisecond,
                Compression:   kafka.CompressionSnappy,
                Retries:       3,
                RequiredAcks:  kafka.RequireAll,
            }),
        ))
        
        // Get Kafka manager
        kafkaManager := forge.GetService[kafka.Manager](app.Container())
        
        // Create producer
        producer, err := kafkaManager.NewProducer("user-events")
        if err != nil {
            panic(err)
        }
        defer producer.Close()
        
        // Produce messages
        ctx := context.Background()
        
        // Single message
        userEvent := UserEvent{
            ID:        "user-123",
            Action:    "created",
            Timestamp: time.Now(),
            Data: map[string]interface{}{
                "email": "user@example.com",
                "name":  "John Doe",
            },
        }
        
        message, err := json.Marshal(userEvent)
        if err != nil {
            panic(err)
        }
        
        err = producer.Send(ctx, kafka.Message{
            Key:   []byte(userEvent.ID),
            Value: message,
            Headers: map[string][]byte{
                "event-type": []byte("user.created"),
                "version":    []byte("v1"),
            },
        })
        if err != nil {
            panic(err)
        }
        
        // Batch messages
        messages := []kafka.Message{}
        for i := 0; i < 100; i++ {
            event := UserEvent{
                ID:        fmt.Sprintf("user-%d", i),
                Action:    "updated",
                Timestamp: time.Now(),
            }
            
            data, _ := json.Marshal(event)
            messages = append(messages, kafka.Message{
                Key:   []byte(event.ID),
                Value: data,
                Headers: map[string][]byte{
                    "event-type": []byte("user.updated"),
                },
            })
        }
        
        err = producer.SendBatch(ctx, messages)
        if err != nil {
            panic(err)
        }
        
        // Async producer with callback
        asyncProducer, err := kafkaManager.NewAsyncProducer("user-events")
        if err != nil {
            panic(err)
        }
        defer asyncProducer.Close()
        
        // Send async with callback
        asyncProducer.SendAsync(kafka.Message{
            Key:   []byte("async-key"),
            Value: []byte("async message"),
        }, func(msg kafka.Message, err error) {
            if err != nil {
                log.Error("failed to send message", "error", err)
            } else {
                log.Info("message sent successfully", 
                    "partition", msg.Partition, 
                    "offset", msg.Offset)
            }
        })
        
        app.Run()
    }
    
    type UserEvent struct {
        ID        string                 `json:"id"`
        Action    string                 `json:"action"`
        Timestamp time.Time              `json:"timestamp"`
        Data      map[string]interface{} `json:"data,omitempty"`
    }
    ```
  </Tab>
  <Tab value="Consumer">
    ```go
    package main
    
    import (
        "context"
        "encoding/json"
        "log"
        
        "github.com/xraph/forge/v2"
        "github.com/xraph/forge/v2/extensions/kafka"
    )
    
    func main() {
        app := forge.NewApp(forge.AppConfig{
            Name: "kafka-consumer",
        })
        
        // Register Kafka extension
        app.RegisterExtension(kafka.NewExtension(
            kafka.WithBrokers([]string{"localhost:9092"}),
            kafka.WithConsumer(kafka.ConsumerConfig{
                Topic:         "user-events",
                GroupID:       "user-service",
                StartOffset:   kafka.LastOffset,
                CommitInterval: 1 * time.Second,
                FetchMin:      1,
                FetchMax:      1024 * 1024, // 1MB
            }),
        ))
        
        kafkaManager := forge.GetService[kafka.Manager](app.Container())
        
        // Create consumer
        consumer, err := kafkaManager.NewConsumer("user-events", "user-service")
        if err != nil {
            panic(err)
        }
        defer consumer.Close()
        
        // Message handler
        handler := &UserEventHandler{
            userService: NewUserService(),
        }
        
        // Start consuming
        ctx := context.Background()
        err = consumer.Consume(ctx, handler.HandleMessage)
        if err != nil {
            panic(err)
        }
        
        app.Run()
    }
    
    type UserEventHandler struct {
        userService *UserService
    }
    
    func (h *UserEventHandler) HandleMessage(ctx context.Context, msg kafka.Message) error {
        // Extract event type from headers
        eventType := string(msg.Headers["event-type"])
        
        log.Printf("Processing message: topic=%s, partition=%d, offset=%d, key=%s, event-type=%s",
            msg.Topic, msg.Partition, msg.Offset, string(msg.Key), eventType)
        
        // Parse message
        var userEvent UserEvent
        if err := json.Unmarshal(msg.Value, &userEvent); err != nil {
            log.Printf("failed to unmarshal message: %v", err)
            return err // This will cause the message to be retried
        }
        
        // Process based on event type
        switch eventType {
        case "user.created":
            return h.handleUserCreated(ctx, userEvent)
        case "user.updated":
            return h.handleUserUpdated(ctx, userEvent)
        case "user.deleted":
            return h.handleUserDeleted(ctx, userEvent)
        default:
            log.Printf("unknown event type: %s", eventType)
            return nil // Skip unknown events
        }
    }
    
    func (h *UserEventHandler) handleUserCreated(ctx context.Context, event UserEvent) error {
        log.Printf("User created: %s", event.ID)
        
        // Process user creation
        user := User{
            ID:        event.ID,
            Email:     event.Data["email"].(string),
            Name:      event.Data["name"].(string),
            CreatedAt: event.Timestamp,
        }
        
        return h.userService.CreateUser(ctx, user)
    }
    
    func (h *UserEventHandler) handleUserUpdated(ctx context.Context, event UserEvent) error {
        log.Printf("User updated: %s", event.ID)
        
        // Process user update
        updates := make(map[string]interface{})
        for k, v := range event.Data {
            updates[k] = v
        }
        
        return h.userService.UpdateUser(ctx, event.ID, updates)
    }
    
    func (h *UserEventHandler) handleUserDeleted(ctx context.Context, event UserEvent) error {
        log.Printf("User deleted: %s", event.ID)
        
        return h.userService.DeleteUser(ctx, event.ID)
    }
    ```
  </Tab>
  <Tab value="Consumer Group">
    ```go
    package main
    
    import (
        "context"
        "sync"
        "time"
        
        "github.com/xraph/forge/v2"
        "github.com/xraph/forge/v2/extensions/kafka"
    )
    
    func main() {
        app := forge.NewApp(forge.AppConfig{
            Name: "kafka-consumer-group",
        })
        
        app.RegisterExtension(kafka.NewExtension(
            kafka.WithBrokers([]string{"localhost:9092"}),
            kafka.WithConsumerGroup(kafka.ConsumerGroupConfig{
                GroupID: "processing-service",
                Topics:  []string{"user-events", "order-events", "payment-events"},
                Config: kafka.ConsumerConfig{
                    StartOffset:    kafka.FirstOffset,
                    CommitInterval: 5 * time.Second,
                    SessionTimeout: 30 * time.Second,
                    HeartbeatInterval: 3 * time.Second,
                    RebalanceTimeout: 30 * time.Second,
                    FetchMin: 1,
                    FetchMax: 10 * 1024 * 1024, // 10MB
                },
            }),
        ))
        
        kafkaManager := forge.GetService[kafka.Manager](app.Container())
        
        // Create consumer group
        consumerGroup, err := kafkaManager.NewConsumerGroup("processing-service")
        if err != nil {
            panic(err)
        }
        defer consumerGroup.Close()
        
        // Create message handlers for different topics
        handlers := map[string]kafka.MessageHandler{
            "user-events":    &UserEventHandler{},
            "order-events":   &OrderEventHandler{},
            "payment-events": &PaymentEventHandler{},
        }
        
        // Start consuming with multiple workers
        ctx, cancel := context.WithCancel(context.Background())
        defer cancel()
        
        var wg sync.WaitGroup
        
        // Start multiple consumer instances for parallel processing
        for i := 0; i < 3; i++ {
            wg.Add(1)
            go func(workerID int) {
                defer wg.Done()
                
                log.Printf("Starting consumer worker %d", workerID)
                
                err := consumerGroup.ConsumeWithHandlers(ctx, handlers)
                if err != nil {
                    log.Printf("Consumer worker %d error: %v", workerID, err)
                }
            }(i)
        }
        
        // Graceful shutdown
        go func() {
            <-app.Context().Done()
            log.Println("Shutting down consumer group...")
            cancel()
        }()
        
        wg.Wait()
        log.Println("All consumer workers stopped")
    }
    
    type UserEventHandler struct{}
    
    func (h *UserEventHandler) HandleMessage(ctx context.Context, msg kafka.Message) error {
        log.Printf("Processing user event: partition=%d, offset=%d", msg.Partition, msg.Offset)
        
        // Simulate processing time
        time.Sleep(100 * time.Millisecond)
        
        // Process the message
        return processUserEvent(ctx, msg)
    }
    
    type OrderEventHandler struct{}
    
    func (h *OrderEventHandler) HandleMessage(ctx context.Context, msg kafka.Message) error {
        log.Printf("Processing order event: partition=%d, offset=%d", msg.Partition, msg.Offset)
        
        // Simulate processing time
        time.Sleep(200 * time.Millisecond)
        
        return processOrderEvent(ctx, msg)
    }
    
    type PaymentEventHandler struct{}
    
    func (h *PaymentEventHandler) HandleMessage(ctx context.Context, msg kafka.Message) error {
        log.Printf("Processing payment event: partition=%d, offset=%d", msg.Partition, msg.Offset)
        
        // Simulate processing time
        time.Sleep(150 * time.Millisecond)
        
        return processPaymentEvent(ctx, msg)
    }
    
    // Advanced consumer group with custom partition assignment
    func setupAdvancedConsumerGroup() {
        app := forge.NewApp(forge.AppConfig{Name: "advanced-consumer"})
        
        app.RegisterExtension(kafka.NewExtension(
            kafka.WithBrokers([]string{"localhost:9092"}),
            kafka.WithConsumerGroup(kafka.ConsumerGroupConfig{
                GroupID: "advanced-processing",
                Topics:  []string{"high-priority", "low-priority"},
                Config: kafka.ConsumerConfig{
                    StartOffset: kafka.FirstOffset,
                    CommitInterval: 1 * time.Second,
                },
                PartitionAssignor: kafka.RoundRobinAssignor,
                RebalanceStrategy: kafka.RebalanceStrategyCooperativeSticky,
            }),
        ))
        
        kafkaManager := forge.GetService[kafka.Manager](app.Container())
        consumerGroup, _ := kafkaManager.NewConsumerGroup("advanced-processing")
        
        // Custom rebalance listener
        consumerGroup.SetRebalanceListener(&CustomRebalanceListener{})
        
        ctx := context.Background()
        
        // Consume with custom message processing
        err := consumerGroup.ConsumeWithProcessor(ctx, &AdvancedMessageProcessor{
            highPriorityQueue: make(chan kafka.Message, 1000),
            lowPriorityQueue:  make(chan kafka.Message, 100),
        })
        if err != nil {
            log.Fatal(err)
        }
    }
    
    type CustomRebalanceListener struct{}
    
    func (l *CustomRebalanceListener) OnPartitionsRevoked(partitions []kafka.TopicPartition) {
        log.Printf("Partitions revoked: %v", partitions)
        // Cleanup resources, commit offsets, etc.
    }
    
    func (l *CustomRebalanceListener) OnPartitionsAssigned(partitions []kafka.TopicPartition) {
        log.Printf("Partitions assigned: %v", partitions)
        // Initialize resources for new partitions
    }
    
    type AdvancedMessageProcessor struct {
        highPriorityQueue chan kafka.Message
        lowPriorityQueue  chan kafka.Message
    }
    
    func (p *AdvancedMessageProcessor) ProcessMessage(ctx context.Context, msg kafka.Message) error {
        // Route messages to different queues based on topic
        switch msg.Topic {
        case "high-priority":
            select {
            case p.highPriorityQueue <- msg:
            case <-ctx.Done():
                return ctx.Err()
            }
        case "low-priority":
            select {
            case p.lowPriorityQueue <- msg:
            case <-ctx.Done():
                return ctx.Err()
            }
        }
        
        return nil
    }
    ```
  </Tab>
  <Tab value="Streaming">
    ```go
    package main
    
    import (
        "context"
        "encoding/json"
        "time"
        
        "github.com/xraph/forge/v2"
        "github.com/xraph/forge/v2/extensions/kafka"
    )
    
    func main() {
        app := forge.NewApp(forge.AppConfig{
            Name: "kafka-streams",
        })
        
        app.RegisterExtension(kafka.NewExtension(
            kafka.WithBrokers([]string{"localhost:9092"}),
            kafka.WithStreams(kafka.StreamsConfig{
                ApplicationID: "user-analytics",
                BootstrapServers: []string{"localhost:9092"},
                DefaultKeySerDe:   kafka.StringSerDe,
                DefaultValueSerDe: kafka.JSONSerDe,
                CommitInterval:    1 * time.Second,
                CacheMaxBytes:     10 * 1024 * 1024, // 10MB
                NumStreamThreads:  4,
            }),
        ))
        
        kafkaManager := forge.GetService[kafka.Manager](app.Container())
        
        // Create stream processor
        streamProcessor, err := kafkaManager.NewStreamProcessor("user-analytics")
        if err != nil {
            panic(err)
        }
        defer streamProcessor.Close()
        
        // Define stream topology
        topology := streamProcessor.NewTopology()
        
        // Source stream
        userEvents := topology.Source("user-events", kafka.StringDeserializer, kafka.JSONDeserializer)
        
        // Filter active users
        activeUsers := userEvents.Filter(func(key string, value interface{}) bool {
            event, ok := value.(map[string]interface{})
            if !ok {
                return false
            }
            
            action, ok := event["action"].(string)
            return ok && (action == "login" || action == "page_view" || action == "purchase")
        })
        
        // Transform events
        enrichedEvents := activeUsers.MapValues(func(value interface{}) interface{} {
            event := value.(map[string]interface{})
            
            // Add enrichment data
            event["processed_at"] = time.Now().Unix()
            event["enriched"] = true
            
            // Add user segment based on action
            action := event["action"].(string)
            switch action {
            case "purchase":
                event["segment"] = "buyer"
            case "login":
                event["segment"] = "active"
            default:
                event["segment"] = "visitor"
            }
            
            return event
        })
        
        // Group by user ID and window
        userSessions := enrichedEvents.
            GroupByKey().
            WindowedBy(kafka.TimeWindows{
                Size:        30 * time.Minute,
                AdvanceBy:   5 * time.Minute,
                GracePeriod: 5 * time.Minute,
            }).
            Aggregate(
                func() interface{} {
                    return UserSession{
                        Events:    []map[string]interface{}{},
                        StartTime: time.Now(),
                    }
                },
                func(key string, value interface{}, aggregate interface{}) interface{} {
                    session := aggregate.(UserSession)
                    event := value.(map[string]interface{})
                    
                    session.Events = append(session.Events, event)
                    session.EventCount++
                    session.LastActivity = time.Now()
                    
                    // Update session metrics
                    if action, ok := event["action"].(string); ok {
                        switch action {
                        case "page_view":
                            session.PageViews++
                        case "purchase":
                            session.Purchases++
                        }
                    }
                    
                    return session
                },
            )
        
        // Output to different topics based on session characteristics
        userSessions.ToStream().
            Branch(
                // High-value sessions
                func(key string, value interface{}) bool {
                    session := value.(UserSession)
                    return session.Purchases > 0 || session.PageViews > 10
                },
                // Regular sessions
                func(key string, value interface{}) bool {
                    return true
                },
            ).
            To([]string{"high-value-sessions", "regular-sessions"})
        
        // Real-time aggregations
        userMetrics := activeUsers.
            GroupBy(func(key string, value interface{}) string {
                event := value.(map[string]interface{})
                action, _ := event["action"].(string)
                return action
            }).
            WindowedBy(kafka.TumblingTimeWindows{Size: 1 * time.Minute}).
            Count()
        
        // Output metrics
        userMetrics.ToStream().To("user-metrics")
        
        // Join with user profile data
        userProfiles := topology.Source("user-profiles", kafka.StringDeserializer, kafka.JSONDeserializer)
        
        enrichedWithProfile := enrichedEvents.
            LeftJoin(userProfiles.ToTable(), func(eventValue, profileValue interface{}) interface{} {
                event := eventValue.(map[string]interface{})
                
                if profileValue != nil {
                    profile := profileValue.(map[string]interface{})
                    event["user_profile"] = profile
                    
                    // Add profile-based enrichment
                    if tier, ok := profile["tier"].(string); ok {
                        event["user_tier"] = tier
                    }
                }
                
                return event
            })
        
        enrichedWithProfile.To("enriched-user-events")
        
        // Start stream processing
        ctx := context.Background()
        err = streamProcessor.Start(ctx, topology)
        if err != nil {
            panic(err)
        }
        
        app.Run()
    }
    
    type UserSession struct {
        Events       []map[string]interface{} `json:"events"`
        EventCount   int                      `json:"event_count"`
        PageViews    int                      `json:"page_views"`
        Purchases    int                      `json:"purchases"`
        StartTime    time.Time                `json:"start_time"`
        LastActivity time.Time                `json:"last_activity"`
    }
    
    // Advanced stream processing with custom processors
    func setupAdvancedStreaming() {
        app := forge.NewApp(forge.AppConfig{Name: "advanced-streams"})
        
        app.RegisterExtension(kafka.NewExtension(
            kafka.WithStreams(kafka.StreamsConfig{
                ApplicationID: "fraud-detection",
                BootstrapServers: []string{"localhost:9092"},
                ProcessingGuarantee: kafka.ExactlyOnce,
                CommitInterval: 100 * time.Millisecond,
            }),
        ))
        
        kafkaManager := forge.GetService[kafka.Manager](app.Container())
        streamProcessor, _ := kafkaManager.NewStreamProcessor("fraud-detection")
        
        topology := streamProcessor.NewTopology()
        
        // Custom processor for fraud detection
        fraudProcessor := &FraudDetectionProcessor{
            riskScoreThreshold: 0.8,
            velocityWindow:     5 * time.Minute,
        }
        
        topology.AddProcessor("fraud-detector", fraudProcessor, "transactions")
        topology.AddSink("alerts", "fraud-alerts", "fraud-detector")
        topology.AddSink("clean", "clean-transactions", "fraud-detector")
        
        streamProcessor.Start(context.Background(), topology)
    }
    
    type FraudDetectionProcessor struct {
        riskScoreThreshold float64
        velocityWindow     time.Duration
        userTransactions   map[string][]Transaction
    }
    
    func (p *FraudDetectionProcessor) Process(key string, value interface{}) []kafka.ProcessorResult {
        transaction := value.(Transaction)
        
        // Calculate risk score
        riskScore := p.calculateRiskScore(transaction)
        
        if riskScore > p.riskScoreThreshold {
            // High risk - send to alerts
            alert := FraudAlert{
                TransactionID: transaction.ID,
                UserID:        transaction.UserID,
                RiskScore:     riskScore,
                Reason:        p.getRiskReason(transaction),
                Timestamp:     time.Now(),
            }
            
            return []kafka.ProcessorResult{
                {
                    Topic: "fraud-alerts",
                    Key:   key,
                    Value: alert,
                },
            }
        }
        
        // Low risk - send to clean transactions
        return []kafka.ProcessorResult{
            {
                Topic: "clean-transactions",
                Key:   key,
                Value: transaction,
            },
        }
    }
    
    func (p *FraudDetectionProcessor) calculateRiskScore(transaction Transaction) float64 {
        score := 0.0
        
        // Check transaction amount
        if transaction.Amount > 10000 {
            score += 0.3
        }
        
        // Check velocity (number of transactions in time window)
        userTxns := p.userTransactions[transaction.UserID]
        recentTxns := 0
        cutoff := time.Now().Add(-p.velocityWindow)
        
        for _, txn := range userTxns {
            if txn.Timestamp.After(cutoff) {
                recentTxns++
            }
        }
        
        if recentTxns > 10 {
            score += 0.4
        }
        
        // Check location
        if transaction.Country != transaction.UserCountry {
            score += 0.2
        }
        
        // Check time of day
        hour := transaction.Timestamp.Hour()
        if hour < 6 || hour > 22 {
            score += 0.1
        }
        
        return score
    }
    
    type Transaction struct {
        ID          string    `json:"id"`
        UserID      string    `json:"user_id"`
        Amount      float64   `json:"amount"`
        Currency    string    `json:"currency"`
        Country     string    `json:"country"`
        UserCountry string    `json:"user_country"`
        Timestamp   time.Time `json:"timestamp"`
    }
    
    type FraudAlert struct {
        TransactionID string    `json:"transaction_id"`
        UserID        string    `json:"user_id"`
        RiskScore     float64   `json:"risk_score"`
        Reason        string    `json:"reason"`
        Timestamp     time.Time `json:"timestamp"`
    }
    ```
  </Tab>
  <Tab value="Transactions">
    ```go
    package main
    
    import (
        "context"
        "encoding/json"
        "fmt"
        
        "github.com/xraph/forge/v2"
        "github.com/xraph/forge/v2/extensions/kafka"
        "github.com/xraph/forge/v2/extensions/database"
    )
    
    func main() {
        app := forge.NewApp(forge.AppConfig{
            Name: "kafka-transactions",
        })
        
        // Register extensions
        app.RegisterExtension(database.NewExtension(
            database.WithPostgreSQL("postgres://localhost/mydb"),
        ))
        
        app.RegisterExtension(kafka.NewExtension(
            kafka.WithBrokers([]string{"localhost:9092"}),
            kafka.WithTransactionalProducer(kafka.TransactionalProducerConfig{
                TransactionalID: "order-processor",
                TransactionTimeout: 30 * time.Second,
                Retries: 3,
                EnableIdempotence: true,
            }),
        ))
        
        kafkaManager := forge.GetService[kafka.Manager](app.Container())
        dbManager := forge.GetService[database.Manager](app.Container())
        
        // Create transactional producer
        txProducer, err := kafkaManager.NewTransactionalProducer("order-processor")
        if err != nil {
            panic(err)
        }
        defer txProducer.Close()
        
        // Process orders with exactly-once semantics
        orderProcessor := &OrderProcessor{
            producer: txProducer,
            db:       dbManager,
        }
        
        // Simulate order processing
        ctx := context.Background()
        
        order := Order{
            ID:       "order-123",
            UserID:   "user-456",
            Items:    []OrderItem{{ProductID: "prod-1", Quantity: 2, Price: 29.99}},
            Total:    59.98,
            Status:   "pending",
        }
        
        err = orderProcessor.ProcessOrder(ctx, order)
        if err != nil {
            log.Fatal("Failed to process order:", err)
        }
        
        app.Run()
    }
    
    type OrderProcessor struct {
        producer kafka.TransactionalProducer
        db       database.Manager
    }
    
    func (p *OrderProcessor) ProcessOrder(ctx context.Context, order Order) error {
        // Begin Kafka transaction
        err := p.producer.BeginTransaction(ctx)
        if err != nil {
            return fmt.Errorf("failed to begin transaction: %w", err)
        }
        
        // Begin database transaction
        dbTx, err := p.db.Begin(ctx)
        if err != nil {
            p.producer.AbortTransaction(ctx)
            return fmt.Errorf("failed to begin database transaction: %w", err)
        }
        
        defer func() {
            if err != nil {
                dbTx.Rollback()
                p.producer.AbortTransaction(ctx)
            }
        }()
        
        // 1. Validate inventory
        for _, item := range order.Items {
            available, err := p.checkInventory(ctx, dbTx, item.ProductID, item.Quantity)
            if err != nil {
                return fmt.Errorf("failed to check inventory: %w", err)
            }
            if !available {
                return fmt.Errorf("insufficient inventory for product %s", item.ProductID)
            }
        }
        
        // 2. Reserve inventory
        for _, item := range order.Items {
            err = p.reserveInventory(ctx, dbTx, item.ProductID, item.Quantity)
            if err != nil {
                return fmt.Errorf("failed to reserve inventory: %w", err)
            }
        }
        
        // 3. Create order in database
        err = p.createOrder(ctx, dbTx, order)
        if err != nil {
            return fmt.Errorf("failed to create order: %w", err)
        }
        
        // 4. Send events to Kafka
        events := []kafka.Message{
            {
                Topic: "order-events",
                Key:   []byte(order.ID),
                Value: p.marshalEvent(OrderCreatedEvent{
                    OrderID:   order.ID,
                    UserID:    order.UserID,
                    Total:     order.Total,
                    Timestamp: time.Now(),
                }),
                Headers: map[string][]byte{
                    "event-type": []byte("order.created"),
                    "version":    []byte("v1"),
                },
            },
        }
        
        for _, item := range order.Items {
            events = append(events, kafka.Message{
                Topic: "inventory-events",
                Key:   []byte(item.ProductID),
                Value: p.marshalEvent(InventoryReservedEvent{
                    ProductID: item.ProductID,
                    Quantity:  item.Quantity,
                    OrderID:   order.ID,
                    Timestamp: time.Now(),
                }),
                Headers: map[string][]byte{
                    "event-type": []byte("inventory.reserved"),
                    "version":    []byte("v1"),
                },
            })
        }
        
        // Send all events in the transaction
        err = p.producer.SendBatch(ctx, events)
        if err != nil {
            return fmt.Errorf("failed to send events: %w", err)
        }
        
        // 5. Commit database transaction
        err = dbTx.Commit()
        if err != nil {
            return fmt.Errorf("failed to commit database transaction: %w", err)
        }
        
        // 6. Commit Kafka transaction
        err = p.producer.CommitTransaction(ctx)
        if err != nil {
            return fmt.Errorf("failed to commit Kafka transaction: %w", err)
        }
        
        log.Printf("Order %s processed successfully", order.ID)
        return nil
    }
    
    func (p *OrderProcessor) checkInventory(ctx context.Context, tx database.Tx, productID string, quantity int) (bool, error) {
        var available int
        err := tx.QueryRow(ctx,
            "SELECT quantity FROM inventory WHERE product_id = $1 FOR UPDATE",
            productID,
        ).Scan(&available)
        
        if err != nil {
            return false, err
        }
        
        return available >= quantity, nil
    }
    
    func (p *OrderProcessor) reserveInventory(ctx context.Context, tx database.Tx, productID string, quantity int) error {
        _, err := tx.Exec(ctx,
            "UPDATE inventory SET quantity = quantity - $1, reserved = reserved + $1 WHERE product_id = $2",
            quantity, productID,
        )
        return err
    }
    
    func (p *OrderProcessor) createOrder(ctx context.Context, tx database.Tx, order Order) error {
        // Insert order
        _, err := tx.Exec(ctx,
            "INSERT INTO orders (id, user_id, total, status, created_at) VALUES ($1, $2, $3, $4, $5)",
            order.ID, order.UserID, order.Total, order.Status, time.Now(),
        )
        if err != nil {
            return err
        }
        
        // Insert order items
        for _, item := range order.Items {
            _, err = tx.Exec(ctx,
                "INSERT INTO order_items (order_id, product_id, quantity, price) VALUES ($1, $2, $3, $4)",
                order.ID, item.ProductID, item.Quantity, item.Price,
            )
            if err != nil {
                return err
            }
        }
        
        return nil
    }
    
    func (p *OrderProcessor) marshalEvent(event interface{}) []byte {
        data, _ := json.Marshal(event)
        return data
    }
    
    // Transactional consumer for exactly-once processing
    func setupTransactionalConsumer() {
        app := forge.NewApp(forge.AppConfig{Name: "tx-consumer"})
        
        app.RegisterExtension(kafka.NewExtension(
            kafka.WithBrokers([]string{"localhost:9092"}),
            kafka.WithTransactionalConsumer(kafka.TransactionalConsumerConfig{
                GroupID: "order-fulfillment",
                Topics:  []string{"order-events"},
                IsolationLevel: kafka.ReadCommitted,
                TransactionalProducer: kafka.TransactionalProducerConfig{
                    TransactionalID: "fulfillment-processor",
                },
            }),
        ))
        
        kafkaManager := forge.GetService[kafka.Manager](app.Container())
        
        txConsumer, err := kafkaManager.NewTransactionalConsumer("order-fulfillment")
        if err != nil {
            panic(err)
        }
        
        ctx := context.Background()
        
        // Process messages with exactly-once semantics
        err = txConsumer.ConsumeTransactionally(ctx, func(ctx context.Context, msg kafka.Message) error {
            // Process the message and produce output atomically
            return processFulfillmentEvent(ctx, msg, txConsumer.Producer())
        })
        
        if err != nil {
            panic(err)
        }
    }
    
    func processFulfillmentEvent(ctx context.Context, msg kafka.Message, producer kafka.TransactionalProducer) error {
        var orderEvent OrderCreatedEvent
        if err := json.Unmarshal(msg.Value, &orderEvent); err != nil {
            return err
        }
        
        // Process fulfillment logic
        fulfillment := FulfillmentEvent{
            OrderID:     orderEvent.OrderID,
            Status:      "processing",
            EstimatedDelivery: time.Now().Add(3 * 24 * time.Hour),
            Timestamp:   time.Now(),
        }
        
        // Send fulfillment event
        fulfillmentData, _ := json.Marshal(fulfillment)
        
        return producer.Send(ctx, kafka.Message{
            Topic: "fulfillment-events",
            Key:   []byte(orderEvent.OrderID),
            Value: fulfillmentData,
            Headers: map[string][]byte{
                "event-type": []byte("fulfillment.started"),
            },
        })
    }
    
    type Order struct {
        ID     string      `json:"id"`
        UserID string      `json:"user_id"`
        Items  []OrderItem `json:"items"`
        Total  float64     `json:"total"`
        Status string      `json:"status"`
    }
    
    type OrderItem struct {
        ProductID string  `json:"product_id"`
        Quantity  int     `json:"quantity"`
        Price     float64 `json:"price"`
    }
    
    type OrderCreatedEvent struct {
        OrderID   string    `json:"order_id"`
        UserID    string    `json:"user_id"`
        Total     float64   `json:"total"`
        Timestamp time.Time `json:"timestamp"`
    }
    
    type InventoryReservedEvent struct {
        ProductID string    `json:"product_id"`
        Quantity  int       `json:"quantity"`
        OrderID   string    `json:"order_id"`
        Timestamp time.Time `json:"timestamp"`
    }
    
    type FulfillmentEvent struct {
        OrderID           string    `json:"order_id"`
        Status            string    `json:"status"`
        EstimatedDelivery time.Time `json:"estimated_delivery"`
        Timestamp         time.Time `json:"timestamp"`
    }
    ```
  </Tab>
</Tabs>

## Configuration

### YAML Configuration

```yaml
kafka:
  # Broker configuration
  brokers:
    - localhost:9092
    - localhost:9093
    - localhost:9094
  
  # Security configuration
  security:
    protocol: SASL_SSL
    sasl:
      mechanism: SCRAM-SHA-512
      username: ${KAFKA_USERNAME}
      password: ${KAFKA_PASSWORD}
    ssl:
      ca_cert_file: /path/to/ca.pem
      cert_file: /path/to/client.pem
      key_file: /path/to/client-key.pem
      verify_ssl: true
  
  # Producer configuration
  producer:
    # Performance settings
    batch_size: 16384
    linger_ms: 5
    buffer_memory: 33554432
    compression_type: snappy
    
    # Reliability settings
    acks: all
    retries: 2147483647
    max_in_flight_requests_per_connection: 5
    enable_idempotence: true
    
    # Timeout settings
    request_timeout_ms: 30000
    delivery_timeout_ms: 120000
    
    # Topic-specific settings
    topics:
      user-events:
        partitions: 12
        replication_factor: 3
        cleanup_policy: delete
        retention_ms: 604800000 # 7 days
      order-events:
        partitions: 24
        replication_factor: 3
        cleanup_policy: compact
        min_insync_replicas: 2
  
  # Consumer configuration
  consumer:
    # Group settings
    group_id: ${SERVICE_NAME}
    auto_offset_reset: earliest
    enable_auto_commit: false
    
    # Performance settings
    fetch_min_bytes: 1
    fetch_max_wait_ms: 500
    max_partition_fetch_bytes: 1048576
    max_poll_records: 500
    
    # Session settings
    session_timeout_ms: 30000
    heartbeat_interval_ms: 3000
    max_poll_interval_ms: 300000
    
    # Topic subscriptions
    topics:
      - user-events
      - order-events
      - payment-events
    
    # Consumer group settings
    partition_assignment_strategy: cooperative_sticky
    
  # Streams configuration
  streams:
    application_id: ${SERVICE_NAME}-streams
    bootstrap_servers:
      - localhost:9092
    
    # Processing settings
    processing_guarantee: exactly_once_v2
    commit_interval_ms: 1000
    cache_max_bytes_buffering: 10485760
    num_stream_threads: 4
    
    # State store settings
    state_dir: /tmp/kafka-streams
    
    # Topology settings
    default_key_serde: string
    default_value_serde: json
    default_timestamp_extractor: wall_clock_time
    
    # Window settings
    window_store_change_log_additional_retention_ms: 86400000
    
  # Admin configuration
  admin:
    request_timeout_ms: 30000
    
    # Topic management
    auto_create_topics: false
    default_replication_factor: 3
    default_partitions: 12
    
  # Monitoring
  monitoring:
    metrics:
      enabled: true
      interval: 30s
      reporters:
        - prometheus
        - jmx
    
    health_check:
      enabled: true
      timeout: 5s
      
    # JMX settings
    jmx:
      enabled: true
      port: 9999
      
  # Schema Registry (if using Avro/Schema Registry)
  schema_registry:
    url: http://localhost:8081
    basic_auth:
      username: ${SCHEMA_REGISTRY_USERNAME}
      password: ${SCHEMA_REGISTRY_PASSWORD}
    
    # Schema settings
    auto_register_schemas: false
    use_latest_version: true
    
  # Connect (if using Kafka Connect)
  connect:
    url: http://localhost:8083
    
    # Connector configurations
    connectors:
      postgres-source:
        connector_class: io.debezium.connector.postgresql.PostgresConnector
        database_hostname: localhost
        database_port: 5432
        database_user: ${DB_USER}
        database_password: ${DB_PASSWORD}
        database_dbname: mydb
        database_server_name: myserver
        table_include_list: public.users,public.orders
```

## Kafka Manager Interface

The Kafka extension provides a comprehensive manager interface:

```go
type Manager interface {
    // Producer methods
    NewProducer(topic string) (Producer, error)
    NewAsyncProducer(topic string) (AsyncProducer, error)
    NewTransactionalProducer(transactionalID string) (TransactionalProducer, error)
    
    // Consumer methods
    NewConsumer(topic, groupID string) (Consumer, error)
    NewConsumerGroup(groupID string) (ConsumerGroup, error)
    NewTransactionalConsumer(groupID string) (TransactionalConsumer, error)
    
    // Streams methods
    NewStreamProcessor(applicationID string) (StreamProcessor, error)
    
    // Admin methods
    CreateTopic(ctx context.Context, topic TopicConfig) error
    DeleteTopic(ctx context.Context, topic string) error
    ListTopics(ctx context.Context) ([]TopicInfo, error)
    DescribeTopic(ctx context.Context, topic string) (TopicInfo, error)
    
    // Health and metrics
    Health(ctx context.Context) error
    Metrics() map[string]interface{}
    
    // Lifecycle
    Close() error
}

// Producer interfaces
type Producer interface {
    Send(ctx context.Context, msg Message) error
    SendBatch(ctx context.Context, messages []Message) error
    Close() error
}

type AsyncProducer interface {
    SendAsync(msg Message, callback func(Message, error))
    Close() error
}

type TransactionalProducer interface {
    Producer
    BeginTransaction(ctx context.Context) error
    CommitTransaction(ctx context.Context) error
    AbortTransaction(ctx context.Context) error
}

// Consumer interfaces
type Consumer interface {
    Consume(ctx context.Context, handler MessageHandler) error
    Close() error
}

type ConsumerGroup interface {
    ConsumeWithHandlers(ctx context.Context, handlers map[string]MessageHandler) error
    ConsumeWithProcessor(ctx context.Context, processor MessageProcessor) error
    SetRebalanceListener(listener RebalanceListener)
    Close() error
}

type TransactionalConsumer interface {
    ConsumeTransactionally(ctx context.Context, handler TransactionalMessageHandler) error
    Producer() TransactionalProducer
    Close() error
}

// Stream processing
type StreamProcessor interface {
    NewTopology() Topology
    Start(ctx context.Context, topology Topology) error
    Stop() error
    Close() error
}
```

## Testing

The Kafka extension provides comprehensive testing utilities:

```go
func TestKafkaProducer(t *testing.T) {
    app := forge.NewTestApp(t, forge.TestConfig{
        Extensions: []forge.Extension{
            kafka.NewExtension(
                kafka.WithEmbeddedBroker(), // Use embedded Kafka for testing
                kafka.WithProducer(kafka.ProducerConfig{
                    Topic: "test-topic",
                }),
            ),
        },
    })
    
    kafkaManager := forge.GetService[kafka.Manager](app.Container())
    
    producer, err := kafkaManager.NewProducer("test-topic")
    require.NoError(t, err)
    defer producer.Close()
    
    ctx := context.Background()
    
    t.Run("send message", func(t *testing.T) {
        msg := kafka.Message{
            Key:   []byte("test-key"),
            Value: []byte("test-value"),
            Headers: map[string][]byte{
                "test-header": []byte("test-header-value"),
            },
        }
        
        err := producer.Send(ctx, msg)
        assert.NoError(t, err)
    })
    
    t.Run("send batch", func(t *testing.T) {
        messages := make([]kafka.Message, 10)
        for i := 0; i < 10; i++ {
            messages[i] = kafka.Message{
                Key:   []byte(fmt.Sprintf("key-%d", i)),
                Value: []byte(fmt.Sprintf("value-%d", i)),
            }
        }
        
        err := producer.SendBatch(ctx, messages)
        assert.NoError(t, err)
    })
}

func TestKafkaConsumer(t *testing.T) {
    app := forge.NewTestApp(t, forge.TestConfig{
        Extensions: []forge.Extension{
            kafka.NewExtension(
                kafka.WithEmbeddedBroker(),
                kafka.WithConsumer(kafka.ConsumerConfig{
                    Topic:   "test-topic",
                    GroupID: "test-group",
                }),
            ),
        },
    })
    
    kafkaManager := forge.GetService[kafka.Manager](app.Container())
    
    // Create producer to send test messages
    producer, err := kafkaManager.NewProducer("test-topic")
    require.NoError(t, err)
    defer producer.Close()
    
    // Create consumer
    consumer, err := kafkaManager.NewConsumer("test-topic", "test-group")
    require.NoError(t, err)
    defer consumer.Close()
    
    ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
    defer cancel()
    
    // Send test message
    testMsg := kafka.Message{
        Key:   []byte("test-key"),
        Value: []byte("test-value"),
    }
    
    err = producer.Send(ctx, testMsg)
    require.NoError(t, err)
    
    // Consume message
    var receivedMsg kafka.Message
    var wg sync.WaitGroup
    wg.Add(1)
    
    go func() {
        defer wg.Done()
        err := consumer.Consume(ctx, func(ctx context.Context, msg kafka.Message) error {
            receivedMsg = msg
            cancel() // Stop consuming after first message
            return nil
        })
        assert.NoError(t, err)
    }()
    
    wg.Wait()
    
    assert.Equal(t, testMsg.Key, receivedMsg.Key)
    assert.Equal(t, testMsg.Value, receivedMsg.Value)
}

// Integration test with real Kafka
func TestKafkaIntegration(t *testing.T) {
    if testing.Short() {
        t.Skip("Skipping integration test")
    }
    
    app := forge.NewTestApp(t, forge.TestConfig{
        Extensions: []forge.Extension{
            kafka.NewExtension(
                kafka.WithBrokers([]string{"localhost:9092"}),
                kafka.WithProducer(kafka.ProducerConfig{
                    Topic: "integration-test",
                }),
                kafka.WithConsumer(kafka.ConsumerConfig{
                    Topic:   "integration-test",
                    GroupID: "integration-test-group",
                }),
            ),
        },
    })
    
    kafkaManager := forge.GetService[kafka.Manager](app.Container())
    
    // Test producer and consumer together
    producer, err := kafkaManager.NewProducer("integration-test")
    require.NoError(t, err)
    defer producer.Close()
    
    consumer, err := kafkaManager.NewConsumer("integration-test", "integration-test-group")
    require.NoError(t, err)
    defer consumer.Close()
    
    ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
    defer cancel()
    
    // Send multiple messages
    messages := make([]kafka.Message, 100)
    for i := 0; i < 100; i++ {
        messages[i] = kafka.Message{
            Key:   []byte(fmt.Sprintf("key-%d", i)),
            Value: []byte(fmt.Sprintf("value-%d", i)),
        }
    }
    
    err = producer.SendBatch(ctx, messages)
    require.NoError(t, err)
    
    // Consume all messages
    receivedCount := 0
    var mu sync.Mutex
    
    err = consumer.Consume(ctx, func(ctx context.Context, msg kafka.Message) error {
        mu.Lock()
        receivedCount++
        if receivedCount >= 100 {
            cancel()
        }
        mu.Unlock()
        return nil
    })
    
    assert.NoError(t, err)
    assert.Equal(t, 100, receivedCount)
}

// Benchmark tests
func BenchmarkKafkaProducer(b *testing.B) {
    app := forge.NewTestApp(b, forge.TestConfig{
        Extensions: []forge.Extension{
            kafka.NewExtension(
                kafka.WithEmbeddedBroker(),
                kafka.WithProducer(kafka.ProducerConfig{
                    Topic: "benchmark-topic",
                }),
            ),
        },
    })
    
    kafkaManager := forge.GetService[kafka.Manager](app.Container())
    producer, err := kafkaManager.NewProducer("benchmark-topic")
    require.NoError(b, err)
    defer producer.Close()
    
    ctx := context.Background()
    msg := kafka.Message{
        Key:   []byte("benchmark-key"),
        Value: []byte("benchmark-value"),
    }
    
    b.ResetTimer()
    b.RunParallel(func(pb *testing.PB) {
        for pb.Next() {
            err := producer.Send(ctx, msg)
            if err != nil {
                b.Fatal(err)
            }
        }
    })
}
```

## Best Practices

<Callout type="info">
Follow these best practices when working with Kafka.
</Callout>

### Performance

- **Batching**: Use batch operations for higher throughput
- **Compression**: Enable compression (snappy, gzip, lz4, zstd)
- **Partitioning**: Choose appropriate partition keys for load distribution
- **Connection Pooling**: Reuse producers and consumers
- **Async Processing**: Use async producers for non-critical messages

### Reliability

- **Idempotence**: Enable idempotent producers
- **Transactions**: Use transactions for exactly-once semantics
- **Retries**: Configure appropriate retry policies
- **Error Handling**: Implement proper error handling and dead letter queues
- **Monitoring**: Monitor lag, throughput, and error rates

### Security

- **Authentication**: Use SASL for authentication
- **Encryption**: Enable SSL/TLS for data in transit
- **Authorization**: Implement ACLs for topic access
- **Schema Validation**: Use Schema Registry for data validation

## Next Steps

<Cards>
  <Card
    title="📊 Events Extension"
    description="Event-driven architecture patterns"
    href="/docs/extensions/events"
  />
  <Card
    title="🗄️ Database Extension"
    description="Persistent data storage"
    href="/docs/extensions/database"
  />
  <Card
    title="📈 Monitoring Guide"
    description="Monitor your Kafka infrastructure"
    href="/docs/guides/monitoring"
  />
  <Card
    title="🏗️ Event Sourcing Guide"
    description="Implement event sourcing with Kafka"
    href="/docs/guides/event-sourcing"
  />
</Cards>