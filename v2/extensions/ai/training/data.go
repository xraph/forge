package training

import (
	"context"
	"fmt"
	"io"
	"sync"
	"time"

	"github.com/xraph/forge/v2/internal/logger"
)

// Dataset defines the interface for training datasets
type Dataset interface {
	// Basic dataset information
	ID() string
	Name() string
	Type() DatasetType
	Version() string
	Size() int64
	RecordCount() int64

	// Dataset lifecycle
	Prepare(ctx context.Context) error
	Validate(ctx context.Context) error
	Load(ctx context.Context) error
	Unload(ctx context.Context) error

	// Data access
	GetRecords(ctx context.Context, offset, limit int) ([]DataRecord, error)
	GetBatch(ctx context.Context, batchSize int) (DataBatch, error)
	GetSample(ctx context.Context, sampleSize int) ([]DataRecord, error)
	Iterator(ctx context.Context) (DataIterator, error)

	// Data manipulation
	Transform(ctx context.Context, transformer DataTransformer) (Dataset, error)
	Filter(ctx context.Context, filter DataFilter) (Dataset, error)
	Split(ctx context.Context, ratios []float64) ([]Dataset, error)
	Merge(ctx context.Context, other Dataset) (Dataset, error)

	// Metadata and statistics
	GetSchema() DataSchema
	GetStatistics() DataStatistics
	GetMetadata() map[string]interface{}
	GetConfig() DatasetConfig

	// Health and metrics
	HealthCheck(ctx context.Context) error
	GetMetrics() DatasetMetrics
}

// DataManager manages training data and datasets
type DataManager interface {
	// Dataset management
	RegisterDataset(dataset Dataset) error
	UnregisterDataset(datasetID string) error
	GetDataset(datasetID string) (Dataset, error)
	ListDatasets() []Dataset

	// Dataset creation
	CreateDataset(ctx context.Context, config DatasetConfig) (Dataset, error)
	LoadDataset(ctx context.Context, source DataSource) (Dataset, error)
	ImportDataset(ctx context.Context, source DataSource, target DatasetConfig) (Dataset, error)

	// Data sources
	RegisterDataSource(source DataSource) error
	GetDataSource(sourceID string) (DataSource, error)
	ListDataSources() []DataSource

	// Data pipeline
	CreatePipeline(config DataPipelineConfig) (DataPipeline, error)
	ExecutePipeline(ctx context.Context, pipelineID string, input DataPipelineInput) (DataPipelineOutput, error)
}

// DatasetType defines the type of dataset
type DatasetType string

const (
	DatasetTypeTabular    DatasetType = "tabular"
	DatasetTypeImage      DatasetType = "image"
	DatasetTypeText       DatasetType = "text"
	DatasetTypeAudio      DatasetType = "audio"
	DatasetTypeVideo      DatasetType = "video"
	DatasetTypeTimeSeries DatasetType = "time_series"
	DatasetTypeGraph      DatasetType = "graph"
	DatasetTypeMultimodal DatasetType = "multimodal"
)

// DatasetConfig contains dataset configuration
type DatasetConfig struct {
	ID          string                 `json:"id"`
	Name        string                 `json:"name"`
	Type        DatasetType            `json:"type"`
	Version     string                 `json:"version"`
	Description string                 `json:"description"`
	Source      DataSource             `json:"source"`
	Schema      DataSchema             `json:"schema"`
	Validation  ValidationRules        `json:"validation"`
	Processing  ProcessingConfig       `json:"processing"`
	Storage     StorageConfig          `json:"storage"`
	Metadata    map[string]interface{} `json:"metadata"`
	Tags        []string               `json:"tags"`
}

// DataSource represents a data source
type DataSource interface {
	ID() string
	Name() string
	Type() DataSourceType
	Connect(ctx context.Context) error
	Disconnect(ctx context.Context) error
	Read(ctx context.Context, query DataQuery) (DataReader, error)
	Write(ctx context.Context, data DataWriter) error
	GetSchema() DataSchema
	GetMetadata() map[string]interface{}
}

// DataSourceType defines the type of data source
type DataSourceType string

const (
	DataSourceTypeFile     DataSourceType = "file"
	DataSourceTypeDatabase DataSourceType = "database"
	DataSourceTypeAPI      DataSourceType = "api"
	DataSourceTypeStream   DataSourceType = "stream"
	DataSourceTypeCloud    DataSourceType = "cloud"
	DataSourceTypeMemory   DataSourceType = "memory"
)

// DataQuery represents a data query
type DataQuery struct {
	Filter    map[string]interface{} `json:"filter"`
	Sort      []SortField            `json:"sort"`
	Limit     int                    `json:"limit"`
	Offset    int                    `json:"offset"`
	Fields    []string               `json:"fields"`
	Aggregate []AggregateField       `json:"aggregate"`
}

// SortField represents a sort field
type SortField struct {
	Field string `json:"field"`
	Order string `json:"order"` // asc, desc
}

// AggregateField represents an aggregate field
type AggregateField struct {
	Field string `json:"field"`
	Type  string `json:"type"` // sum, avg, count, min, max
	Alias string `json:"alias"`
}

// DataReader defines the interface for reading data
type DataReader interface {
	Read(ctx context.Context) (DataRecord, error)
	ReadBatch(ctx context.Context, batchSize int) ([]DataRecord, error)
	ReadAll(ctx context.Context) ([]DataRecord, error)
	Close() error
	HasNext() bool
}

// DataWriter defines the interface for writing data
type DataWriter interface {
	Write(ctx context.Context, record DataRecord) error
	WriteBatch(ctx context.Context, records []DataRecord) error
	Flush(ctx context.Context) error
	Close() error
}

// DataRecord represents a single data record
type DataRecord struct {
	ID       string                 `json:"id"`
	Data     map[string]interface{} `json:"data"`
	Labels   map[string]interface{} `json:"labels,omitempty"`
	Metadata map[string]interface{} `json:"metadata"`
	Version  string                 `json:"version"`
}

// DataBatch represents a batch of data records
type DataBatch struct {
	ID       string                 `json:"id"`
	Records  []DataRecord           `json:"records"`
	Size     int                    `json:"size"`
	Metadata map[string]interface{} `json:"metadata"`
}

// DataIterator provides iteration over dataset records
type DataIterator interface {
	HasNext() bool
	Next(ctx context.Context) (DataRecord, error)
	Reset() error
	Close() error
	Position() int
	Size() int
}

// DataSchema represents the structure of data
type DataSchema struct {
	Version string                 `json:"version"`
	Fields  map[string]FieldSchema `json:"fields"`
	Labels  map[string]FieldSchema `json:"labels,omitempty"`
	Primary []string               `json:"primary_keys"`
	Foreign map[string]ForeignKey  `json:"foreign_keys,omitempty"`
	Indexes []IndexSchema          `json:"indexes,omitempty"`
}

// FieldSchema defines the schema for a field
type FieldSchema struct {
	Name        string                 `json:"name"`
	Type        string                 `json:"type"`
	Required    bool                   `json:"required"`
	Nullable    bool                   `json:"nullable"`
	Default     interface{}            `json:"default,omitempty"`
	Constraints map[string]interface{} `json:"constraints,omitempty"`
	Format      string                 `json:"format,omitempty"`
	Description string                 `json:"description,omitempty"`
}

// ForeignKey represents a foreign key relationship
type ForeignKey struct {
	Table  string   `json:"table"`
	Fields []string `json:"fields"`
}

// IndexSchema represents an index
type IndexSchema struct {
	Name   string   `json:"name"`
	Fields []string `json:"fields"`
	Unique bool     `json:"unique"`
	Type   string   `json:"type"`
}

// DataStatistics contains dataset statistics
type DataStatistics struct {
	RecordCount  int64                         `json:"record_count"`
	Size         int64                         `json:"size"`
	Fields       map[string]FieldStatistics    `json:"fields"`
	Correlations map[string]map[string]float64 `json:"correlations,omitempty"`
	Distribution map[string]interface{}        `json:"distribution,omitempty"`
	Quality      DataQualityMetrics            `json:"quality"`
	LastUpdated  time.Time                     `json:"last_updated"`
}

// FieldStatistics contains statistics for a field
type FieldStatistics struct {
	Count        int64                  `json:"count"`
	NullCount    int64                  `json:"null_count"`
	UniqueCount  int64                  `json:"unique_count"`
	Mean         *float64               `json:"mean,omitempty"`
	Median       *float64               `json:"median,omitempty"`
	Mode         interface{}            `json:"mode,omitempty"`
	Min          interface{}            `json:"min,omitempty"`
	Max          interface{}            `json:"max,omitempty"`
	StdDev       *float64               `json:"std_dev,omitempty"`
	Variance     *float64               `json:"variance,omitempty"`
	Percentiles  map[string]interface{} `json:"percentiles,omitempty"`
	Distribution map[string]int64       `json:"distribution,omitempty"`
}

// DataQualityMetrics contains data quality metrics
type DataQualityMetrics struct {
	Completeness float64            `json:"completeness"`
	Accuracy     float64            `json:"accuracy"`
	Consistency  float64            `json:"consistency"`
	Validity     float64            `json:"validity"`
	Uniqueness   float64            `json:"uniqueness"`
	Issues       []DataQualityIssue `json:"issues"`
}

// DataQualityIssue represents a data quality issue
type DataQualityIssue struct {
	Type        string        `json:"type"`
	Field       string        `json:"field,omitempty"`
	Message     string        `json:"message"`
	Count       int64         `json:"count"`
	Severity    string        `json:"severity"`
	Examples    []interface{} `json:"examples,omitempty"`
	Suggestions []string      `json:"suggestions,omitempty"`
}

// ValidationRules contains data validation rules
type ValidationRules struct {
	Required   []string               `json:"required"`
	Types      map[string]string      `json:"types"`
	Ranges     map[string]Range       `json:"ranges"`
	Patterns   map[string]string      `json:"patterns"`
	Enums      map[string][]string    `json:"enums"`
	Custom     []CustomValidation     `json:"custom"`
	CrossField []CrossFieldValidation `json:"cross_field"`
}

// Range represents a value range
type Range struct {
	Min interface{} `json:"min,omitempty"`
	Max interface{} `json:"max,omitempty"`
}

// CustomValidation represents custom validation logic
type CustomValidation struct {
	Name       string                 `json:"name"`
	Field      string                 `json:"field"`
	Rule       string                 `json:"rule"`
	Parameters map[string]interface{} `json:"parameters"`
	Message    string                 `json:"message"`
}

// CrossFieldValidation represents validation across multiple fields
type CrossFieldValidation struct {
	Name    string   `json:"name"`
	Fields  []string `json:"fields"`
	Rule    string   `json:"rule"`
	Message string   `json:"message"`
}

// ProcessingConfig contains data processing configuration
type ProcessingConfig struct {
	Transformations []TransformationConfig   `json:"transformations"`
	Normalization   NormalizationConfig      `json:"normalization"`
	Encoding        EncodingConfig           `json:"encoding"`
	FeatureEng      FeatureEngineeringConfig `json:"feature_engineering"`
	Sampling        SamplingConfig           `json:"sampling"`
	Augmentation    AugmentationConfig       `json:"augmentation"`
}

// TransformationConfig contains transformation configuration
type TransformationConfig struct {
	Type       string                 `json:"type"`
	Fields     []string               `json:"fields"`
	Parameters map[string]interface{} `json:"parameters"`
	Output     string                 `json:"output,omitempty"`
}

// NormalizationConfig contains normalization configuration
type NormalizationConfig struct {
	Method string             `json:"method"` // min-max, z-score, robust, quantile
	Fields []string           `json:"fields"`
	Params map[string]float64 `json:"params,omitempty"`
}

// EncodingConfig contains encoding configuration
type EncodingConfig struct {
	Categorical CategoricalEncodingConfig `json:"categorical"`
	Text        TextEncodingConfig        `json:"text"`
	DateTime    DateTimeEncodingConfig    `json:"datetime"`
}

// CategoricalEncodingConfig contains categorical encoding configuration
type CategoricalEncodingConfig struct {
	Method string                 `json:"method"` // one-hot, label, ordinal, target
	Fields []string               `json:"fields"`
	Params map[string]interface{} `json:"params,omitempty"`
}

// TextEncodingConfig contains text encoding configuration
type TextEncodingConfig struct {
	Method     string                 `json:"method"` // tfidf, word2vec, bert
	Fields     []string               `json:"fields"`
	Vocabulary int                    `json:"vocabulary"`
	Params     map[string]interface{} `json:"params,omitempty"`
}

// DateTimeEncodingConfig contains datetime encoding configuration
type DateTimeEncodingConfig struct {
	Fields     []string `json:"fields"`
	Components []string `json:"components"` // year, month, day, hour, minute, dayofweek
	Cyclical   bool     `json:"cyclical"`
}

// FeatureEngineeringConfig contains feature engineering configuration
type FeatureEngineeringConfig struct {
	Create  []FeatureCreation   `json:"create"`
	Select  FeatureSelection    `json:"select"`
	Extract []FeatureExtraction `json:"extract"`
}

// FeatureCreation represents feature creation rules
type FeatureCreation struct {
	Name       string                 `json:"name"`
	Type       string                 `json:"type"` // polynomial, interaction, ratio, etc.
	Fields     []string               `json:"fields"`
	Parameters map[string]interface{} `json:"parameters"`
}

// FeatureSelection represents feature selection configuration
type FeatureSelection struct {
	Method     string                 `json:"method"` // variance, correlation, mutual_info, etc.
	Count      int                    `json:"count,omitempty"`
	Threshold  float64                `json:"threshold,omitempty"`
	Parameters map[string]interface{} `json:"parameters"`
}

// FeatureExtraction represents feature extraction configuration
type FeatureExtraction struct {
	Method     string                 `json:"method"` // pca, lda, ica, etc.
	Components int                    `json:"components"`
	Parameters map[string]interface{} `json:"parameters"`
}

// SamplingConfig contains sampling configuration
type SamplingConfig struct {
	Method   string  `json:"method"` // random, stratified, systematic
	Ratio    float64 `json:"ratio"`
	Seed     int64   `json:"seed"`
	Stratify string  `json:"stratify,omitempty"`
}

// AugmentationConfig contains data augmentation configuration
type AugmentationConfig struct {
	Enabled     bool                    `json:"enabled"`
	Techniques  []AugmentationTechnique `json:"techniques"`
	Probability float64                 `json:"probability"`
}

// AugmentationTechnique represents an augmentation technique
type AugmentationTechnique struct {
	Name       string                 `json:"name"`
	Type       string                 `json:"type"`
	Parameters map[string]interface{} `json:"parameters"`
	Fields     []string               `json:"fields,omitempty"`
}

// StorageConfig contains storage configuration
type StorageConfig struct {
	Type         string                 `json:"type"` // file, database, memory, cloud
	Location     string                 `json:"location"`
	Format       string                 `json:"format"` // csv, json, parquet, hdf5
	Compression  string                 `json:"compression,omitempty"`
	Partitioning PartitioningConfig     `json:"partitioning"`
	Caching      CachingConfig          `json:"caching"`
	Parameters   map[string]interface{} `json:"parameters"`
}

// PartitioningConfig contains partitioning configuration
type PartitioningConfig struct {
	Enabled  bool     `json:"enabled"`
	Fields   []string `json:"fields"`
	Strategy string   `json:"strategy"` // hash, range, list
}

// CachingConfig contains caching configuration
type CachingConfig struct {
	Enabled  bool          `json:"enabled"`
	TTL      time.Duration `json:"ttl"`
	MaxSize  int64         `json:"max_size"`
	Strategy string        `json:"strategy"` // lru, lfu, fifo
}

// DatasetMetrics contains dataset metrics
type DatasetMetrics struct {
	AccessCount    int64         `json:"access_count"`
	LastAccessed   time.Time     `json:"last_accessed"`
	LoadTime       time.Duration `json:"load_time"`
	PrepareTime    time.Duration `json:"prepare_time"`
	ValidationTime time.Duration `json:"validation_time"`
	Size           int64         `json:"size"`
	RecordCount    int64         `json:"record_count"`
	ErrorRate      float64       `json:"error_rate"`
	QualityScore   float64       `json:"quality_score"`
}

// DataTransformer defines the interface for data transformation
type DataTransformer interface {
	Name() string
	Transform(ctx context.Context, record DataRecord) (DataRecord, error)
	BatchTransform(ctx context.Context, records []DataRecord) ([]DataRecord, error)
	Fit(ctx context.Context, dataset Dataset) error
	GetParams() map[string]interface{}
	SetParams(params map[string]interface{}) error
}

// DataFilter defines the interface for data filtering
type DataFilter interface {
	Name() string
	Filter(ctx context.Context, record DataRecord) (bool, error)
	GetCriteria() map[string]interface{}
}

// DataPipeline represents a data processing pipeline
type DataPipeline interface {
	ID() string
	Name() string
	Execute(ctx context.Context, input DataPipelineInput) (DataPipelineOutput, error)
	AddStage(stage DataPipelineStage) error
	GetConfig() DataPipelineConfig
}

// DataPipelineConfig contains data pipeline configuration
type DataPipelineConfig struct {
	ID          string                    `json:"id"`
	Name        string                    `json:"name"`
	Description string                    `json:"description"`
	Stages      []DataPipelineStageConfig `json:"stages"`
	Parameters  map[string]interface{}    `json:"parameters"`
}

// DataPipelineStageConfig contains stage configuration
type DataPipelineStageConfig struct {
	ID         string                 `json:"id"`
	Name       string                 `json:"name"`
	Type       string                 `json:"type"`
	Parameters map[string]interface{} `json:"parameters"`
}

// DataPipelineStage represents a pipeline stage
type DataPipelineStage interface {
	ID() string
	Name() string
	Execute(ctx context.Context, input interface{}) (interface{}, error)
}

// DataPipelineInput represents pipeline input
type DataPipelineInput struct {
	Data       interface{}            `json:"data"`
	Parameters map[string]interface{} `json:"parameters"`
}

// DataPipelineOutput represents pipeline output
type DataPipelineOutput struct {
	Data     interface{}            `json:"data"`
	Metadata map[string]interface{} `json:"metadata"`
	Metrics  map[string]float64     `json:"metrics"`
}

// DatasetImpl implements the Dataset interface
type DatasetImpl struct {
	id         string
	config     DatasetConfig
	schema     DataSchema
	statistics DataStatistics
	metrics    DatasetMetrics
	records    []DataRecord
	loaded     bool
	logger     logger.Logger
	mu         sync.RWMutex
	createdAt  time.Time
}

// NewDataset creates a new dataset
func NewDataset(config DatasetConfig, logger logger.Logger) Dataset {
	return &DatasetImpl{
		id:        config.ID,
		config:    config,
		logger:    logger,
		records:   make([]DataRecord, 0),
		createdAt: time.Now(),
	}
}

// ID returns the dataset ID
func (d *DatasetImpl) ID() string {
	return d.id
}

// Name returns the dataset name
func (d *DatasetImpl) Name() string {
	return d.config.Name
}

// Type returns the dataset type
func (d *DatasetImpl) Type() DatasetType {
	return d.config.Type
}

// Version returns the dataset version
func (d *DatasetImpl) Version() string {
	return d.config.Version
}

// Size returns the dataset size in bytes
func (d *DatasetImpl) Size() int64 {
	d.mu.RLock()
	defer d.mu.RUnlock()
	return d.statistics.Size
}

// RecordCount returns the number of records
func (d *DatasetImpl) RecordCount() int64 {
	d.mu.RLock()
	defer d.mu.RUnlock()
	return int64(len(d.records))
}

// Prepare prepares the dataset for use
func (d *DatasetImpl) Prepare(ctx context.Context) error {
	d.mu.Lock()
	defer d.mu.Unlock()

	startTime := time.Now()

	// Connect to data source
	if err := d.config.Source.Connect(ctx); err != nil {
		return fmt.Errorf("failed to connect to data source: %w", err)
	}

	// Load data from source
	query := DataQuery{} // Load all data
	reader, err := d.config.Source.Read(ctx, query)
	if err != nil {
		return fmt.Errorf("failed to read data: %w", err)
	}
	defer reader.Close()

	// Read all records
	records, err := reader.ReadAll(ctx)
	if err != nil {
		return fmt.Errorf("failed to read records: %w", err)
	}

	d.records = records
	d.metrics.PrepareTime = time.Since(startTime)
	d.metrics.RecordCount = int64(len(records))

	// Calculate statistics
	if err := d.calculateStatistics(); err != nil {
		d.logger.Warn("failed to calculate statistics", logger.Error(err))
	}

	d.logger.Info("dataset prepared",
		logger.String("dataset_id", d.id),
		logger.Int("record_count", len(records)),
		logger.Duration("prepare_time", d.metrics.PrepareTime),
	)

	return nil
}

// Validate validates the dataset
func (d *DatasetImpl) Validate(ctx context.Context) error {
	d.mu.RLock()
	defer d.mu.RUnlock()

	startTime := time.Now()

	// Validate records against schema and rules
	errorCount := 0
	for _, record := range d.records {
		if err := d.validateRecord(record); err != nil {
			errorCount++
		}
	}

	d.metrics.ValidationTime = time.Since(startTime)
	d.metrics.ErrorRate = float64(errorCount) / float64(len(d.records))

	if errorCount > 0 {
		d.logger.Warn("dataset validation found errors",
			logger.String("dataset_id", d.id),
			logger.Int("error_count", errorCount),
			logger.Float64("error_rate", d.metrics.ErrorRate),
		)
	}

	d.logger.Info("dataset validated",
		logger.String("dataset_id", d.id),
		logger.Duration("validation_time", d.metrics.ValidationTime),
	)

	return nil
}

// Load loads the dataset into memory
func (d *DatasetImpl) Load(ctx context.Context) error {
	d.mu.Lock()
	defer d.mu.Unlock()

	if d.loaded {
		return nil
	}

	startTime := time.Now()

	// Load implementation would depend on storage type
	// For now, assume data is already loaded in Prepare

	d.loaded = true
	d.metrics.LoadTime = time.Since(startTime)
	d.metrics.LastAccessed = time.Now()

	d.logger.Info("dataset loaded",
		logger.String("dataset_id", d.id),
		logger.Duration("load_time", d.metrics.LoadTime),
	)

	return nil
}

// Unload unloads the dataset from memory
func (d *DatasetImpl) Unload(ctx context.Context) error {
	d.mu.Lock()
	defer d.mu.Unlock()

	d.loaded = false
	// In a real implementation, this might clear cached data

	d.logger.Info("dataset unloaded",
		logger.String("dataset_id", d.id),
	)

	return nil
}

// GetRecords returns a slice of records
func (d *DatasetImpl) GetRecords(ctx context.Context, offset, limit int) ([]DataRecord, error) {
	d.mu.RLock()
	defer d.mu.RUnlock()

	d.metrics.AccessCount++
	d.metrics.LastAccessed = time.Now()

	if offset >= len(d.records) {
		return []DataRecord{}, nil
	}

	end := offset + limit
	if end > len(d.records) {
		end = len(d.records)
	}

	return d.records[offset:end], nil
}

// GetBatch returns a batch of records
func (d *DatasetImpl) GetBatch(ctx context.Context, batchSize int) (DataBatch, error) {
	records, err := d.GetRecords(ctx, 0, batchSize)
	if err != nil {
		return DataBatch{}, err
	}

	return DataBatch{
		ID:      fmt.Sprintf("batch_%d", time.Now().UnixNano()),
		Records: records,
		Size:    len(records),
		Metadata: map[string]interface{}{
			"dataset_id": d.id,
			"created_at": time.Now(),
		},
	}, nil
}

// GetSample returns a random sample of records
func (d *DatasetImpl) GetSample(ctx context.Context, sampleSize int) ([]DataRecord, error) {
	d.mu.RLock()
	defer d.mu.RUnlock()

	if sampleSize >= len(d.records) {
		return d.records, nil
	}

	// Simple random sampling (in production, use proper sampling)
	sample := make([]DataRecord, sampleSize)
	for i := 0; i < sampleSize; i++ {
		sample[i] = d.records[i%len(d.records)]
	}

	return sample, nil
}

// Iterator returns a data iterator
func (d *DatasetImpl) Iterator(ctx context.Context) (DataIterator, error) {
	return NewDatasetIterator(d), nil
}

// Transform applies a transformation to the dataset
func (d *DatasetImpl) Transform(ctx context.Context, transformer DataTransformer) (Dataset, error) {
	transformedRecords := make([]DataRecord, 0, len(d.records))

	for _, record := range d.records {
		transformed, err := transformer.Transform(ctx, record)
		if err != nil {
			return nil, fmt.Errorf("failed to transform record %s: %w", record.ID, err)
		}
		transformedRecords = append(transformedRecords, transformed)
	}

	// Create new dataset with transformed records
	transformedConfig := d.config
	transformedConfig.ID = d.id + "_transformed"
	transformedDataset := NewDataset(transformedConfig, d.logger).(*DatasetImpl)
	transformedDataset.records = transformedRecords

	return transformedDataset, nil
}

// Filter applies a filter to the dataset
func (d *DatasetImpl) Filter(ctx context.Context, filter DataFilter) (Dataset, error) {
	filteredRecords := make([]DataRecord, 0)

	for _, record := range d.records {
		keep, err := filter.Filter(ctx, record)
		if err != nil {
			return nil, fmt.Errorf("failed to filter record %s: %w", record.ID, err)
		}
		if keep {
			filteredRecords = append(filteredRecords, record)
		}
	}

	// Create new dataset with filtered records
	filteredConfig := d.config
	filteredConfig.ID = d.id + "_filtered"
	filteredDataset := NewDataset(filteredConfig, d.logger).(*DatasetImpl)
	filteredDataset.records = filteredRecords

	return filteredDataset, nil
}

// Split splits the dataset into multiple datasets
func (d *DatasetImpl) Split(ctx context.Context, ratios []float64) ([]Dataset, error) {
	if len(ratios) == 0 {
		return nil, fmt.Errorf("at least one ratio must be specified")
	}

	// Validate ratios sum to 1.0
	sum := 0.0
	for _, ratio := range ratios {
		sum += ratio
	}
	if sum != 1.0 {
		return nil, fmt.Errorf("ratios must sum to 1.0, got %f", sum)
	}

	datasets := make([]Dataset, len(ratios))
	recordCount := len(d.records)
	startIdx := 0

	for i, ratio := range ratios {
		endIdx := startIdx + int(float64(recordCount)*ratio)
		if i == len(ratios)-1 {
			endIdx = recordCount // Ensure we include all records
		}

		splitConfig := d.config
		splitConfig.ID = fmt.Sprintf("%s_split_%d", d.id, i)
		splitDataset := NewDataset(splitConfig, d.logger).(*DatasetImpl)
		splitDataset.records = d.records[startIdx:endIdx]

		datasets[i] = splitDataset
		startIdx = endIdx
	}

	return datasets, nil
}

// Merge merges this dataset with another dataset
func (d *DatasetImpl) Merge(ctx context.Context, other Dataset) (Dataset, error) {
	otherImpl, ok := other.(*DatasetImpl)
	if !ok {
		return nil, fmt.Errorf("can only merge with DatasetImpl")
	}

	mergedRecords := make([]DataRecord, 0, len(d.records)+len(otherImpl.records))
	mergedRecords = append(mergedRecords, d.records...)
	mergedRecords = append(mergedRecords, otherImpl.records...)

	mergedConfig := d.config
	mergedConfig.ID = d.id + "_" + other.ID() + "_merged"
	mergedDataset := NewDataset(mergedConfig, d.logger).(*DatasetImpl)
	mergedDataset.records = mergedRecords

	return mergedDataset, nil
}

// GetSchema returns the dataset schema
func (d *DatasetImpl) GetSchema() DataSchema {
	return d.schema
}

// GetStatistics returns dataset statistics
func (d *DatasetImpl) GetStatistics() DataStatistics {
	d.mu.RLock()
	defer d.mu.RUnlock()
	return d.statistics
}

// GetMetadata returns dataset metadata
func (d *DatasetImpl) GetMetadata() map[string]interface{} {
	return d.config.Metadata
}

// GetConfig returns dataset configuration
func (d *DatasetImpl) GetConfig() DatasetConfig {
	return d.config
}

// HealthCheck performs a health check on the dataset
func (d *DatasetImpl) HealthCheck(ctx context.Context) error {
	d.mu.RLock()
	defer d.mu.RUnlock()

	if !d.loaded {
		return fmt.Errorf("dataset is not loaded")
	}

	if len(d.records) == 0 {
		return fmt.Errorf("dataset is empty")
	}

	if d.metrics.ErrorRate > 0.1 {
		return fmt.Errorf("dataset has high error rate: %f", d.metrics.ErrorRate)
	}

	return nil
}

// GetMetrics returns dataset metrics
func (d *DatasetImpl) GetMetrics() DatasetMetrics {
	d.mu.RLock()
	defer d.mu.RUnlock()
	return d.metrics
}

// Helper methods

func (d *DatasetImpl) calculateStatistics() error {
	if len(d.records) == 0 {
		return nil
	}

	d.statistics = DataStatistics{
		RecordCount: int64(len(d.records)),
		Fields:      make(map[string]FieldStatistics),
		LastUpdated: time.Now(),
	}

	// Calculate basic statistics
	// This is a simplified implementation
	for _, record := range d.records {
		for field, value := range record.Data {
			if _, exists := d.statistics.Fields[field]; !exists {
				d.statistics.Fields[field] = FieldStatistics{
					Distribution: make(map[string]int64),
				}
			}

			stats := d.statistics.Fields[field]
			stats.Count++

			if value == nil {
				stats.NullCount++
			} else {
				// Update distribution
				valueStr := fmt.Sprintf("%v", value)
				stats.Distribution[valueStr]++
			}

			d.statistics.Fields[field] = stats
		}
	}

	// Calculate quality metrics
	d.calculateQualityMetrics()

	return nil
}

func (d *DatasetImpl) calculateQualityMetrics() {
	totalFields := len(d.statistics.Fields)
	if totalFields == 0 {
		return
	}

	var completeness, validity float64
	for _, fieldStats := range d.statistics.Fields {
		if fieldStats.Count > 0 {
			fieldCompleteness := float64(fieldStats.Count-fieldStats.NullCount) / float64(fieldStats.Count)
			completeness += fieldCompleteness
			validity += 1.0 // Simplified - assume all non-null values are valid
		}
	}

	d.statistics.Quality = DataQualityMetrics{
		Completeness: completeness / float64(totalFields),
		Validity:     validity / float64(totalFields),
		Consistency:  1.0, // Simplified
		Accuracy:     1.0, // Simplified
		Uniqueness:   1.0, // Simplified
		Issues:       []DataQualityIssue{},
	}

	d.metrics.QualityScore = (d.statistics.Quality.Completeness +
		d.statistics.Quality.Validity +
		d.statistics.Quality.Consistency +
		d.statistics.Quality.Accuracy +
		d.statistics.Quality.Uniqueness) / 5.0
}

func (d *DatasetImpl) validateRecord(record DataRecord) error {
	// Implement validation logic based on schema and rules
	// This is a simplified implementation
	for _, rule := range d.config.Validation.Required {
		if _, exists := record.Data[d.config.Name]; !exists {
			return fmt.Errorf("required field %s is missing", rule)
		}
	}
	return nil
}

// DatasetIterator implements DataIterator
type DatasetIterator struct {
	dataset  *DatasetImpl
	position int
	size     int
}

func NewDatasetIterator(dataset *DatasetImpl) *DatasetIterator {
	return &DatasetIterator{
		dataset:  dataset,
		position: 0,
		size:     len(dataset.records),
	}
}

func (it *DatasetIterator) HasNext() bool {
	return it.position < it.size
}

func (it *DatasetIterator) Next(ctx context.Context) (DataRecord, error) {
	if !it.HasNext() {
		return DataRecord{}, io.EOF
	}

	record := it.dataset.records[it.position]
	it.position++
	return record, nil
}

func (it *DatasetIterator) Reset() error {
	it.position = 0
	return nil
}

func (it *DatasetIterator) Close() error {
	return nil
}

func (it *DatasetIterator) Position() int {
	return it.position
}

func (it *DatasetIterator) Size() int {
	return it.size
}
